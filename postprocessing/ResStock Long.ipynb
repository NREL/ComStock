{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988acccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textwrap import indent\n",
    "\n",
    "import boto3\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from comstockpostproc.resstock_naming_mixin import ResStockNamingMixin\n",
    "from comstockpostproc.units_mixin import UnitsMixin\n",
    "from comstockpostproc.s3_utilities_mixin import S3UtilitiesMixin\n",
    "from comstockpostproc import ResStock\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab395c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:comstockpostproc.resstock:Creating ResStock 2021r1 2018\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "When getting information for key 'nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_amy2018_release_1/metadata/metadata.parquet' in bucket 'oedi-data-lake': AWS Error ACCESS_DENIED during HeadObject operation: No response body.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resstock \u001b[38;5;241m=\u001b[39m \u001b[43mResStock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_base_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moedi-data-lake/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresstock_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresstock_amy2018_release_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresstock_run_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2021r1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresstock_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2018\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruth_data_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownselect_to_multifamily\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreload_from_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ComStock/postprocessing/comstockpostproc/resstock.py:70\u001b[0m, in \u001b[0;36mResStock.__init__\u001b[0;34m(self, s3_base_dir, resstock_run_name, resstock_run_version, resstock_year, truth_data_version, weighted_energy_units, reload_from_csv, downselect_to_multifamily, multifamily_building_efficiency_ratio)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3_inpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_base_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresstock_run_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Load and transform data, preserving all columns\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reload_from_csv:\n\u001b[1;32m     72\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResStock wide.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ComStock/postprocessing/comstockpostproc/resstock.py:98\u001b[0m, in \u001b[0;36mResStock.download_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(results_data_path):\n\u001b[1;32m     97\u001b[0m     s3_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3_inpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/metadata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#s3_path = f\"{self.s3_inpath}/metatdata_and_annual_results/national/parquet/Baseline/{self.results_file_name}\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     data\u001b[38;5;241m.\u001b[39mto_parquet(results_data_path)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# egrid emissions factors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyarrow/parquet/core.py:1776\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1770\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1771\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1773\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyarrow/parquet/core.py:1329\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m         filesystem \u001b[38;5;241m=\u001b[39m LocalFileSystem(use_mmap\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m-> 1329\u001b[0m finfo \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finfo\u001b[38;5;241m.\u001b[39mis_file:\n\u001b[1;32m   1331\u001b[0m     single_file \u001b[38;5;241m=\u001b[39m path_or_paths\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyarrow/_fs.pyx:581\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.get_file_info\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: When getting information for key 'nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_amy2018_release_1/metadata/metadata.parquet' in bucket 'oedi-data-lake': AWS Error ACCESS_DENIED during HeadObject operation: No response body."
     ]
    }
   ],
   "source": [
    "# Failing test\n",
    "resstock = ResStock(\n",
    "    s3_base_dir='oedi-data-lake/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021',\n",
    "    resstock_run_name='resstock_amy2018_release_1',\n",
    "    resstock_run_version='2021r1',\n",
    "    resstock_year=2018,\n",
    "    truth_data_version='v01',\n",
    "    downselect_to_multifamily=False,\n",
    "    reload_from_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf6e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResStock_2(ResStockNamingMixin, UnitsMixin, S3UtilitiesMixin):\n",
    "    def __init__(self, resstock_file_name, resstock_run_version, resstock_year, truth_data_version='v01',\n",
    "        weighted_energy_units='tbtu', reload_from_csv=False, downselect_to_multifamily=True,\n",
    "        multifamily_building_efficiency_ratio=0.75):\n",
    "        \"\"\"\n",
    "        A class to load and transform ResStock multifamily data for export, analysis, and comparison.\n",
    "        Set up to pull from postprocessed results from S3 OEDI bucket, not raw ResStock results from S3 RESBLDG.\n",
    "        Args:\n",
    "\n",
    "            resstock_run_s3_dir (str): The location of the ResStock run on S3 in the OEDI bucket\n",
    "            resstock_run_name (str): The name of the ResStock run, used to look\n",
    "            up the data on S3\n",
    "            resstock_year (int): The year represented by this ResStock run\n",
    "            resstock_run_version (str): The version string for this ResStock run\n",
    "            to differentiate it from other ResStock runs\n",
    "            downselect_to_multifamily (bool): If True, drops all non-multifamily results\n",
    "            multifamily_building_efficiency_ratio (float): 0-1 fraction of rentable area (units) to gross area,\n",
    "            which includes common areas such as corridors, lobbies, gyms, etc.\n",
    "            Typical ratio is 75% for multfamily per several sources such as\n",
    "            https://multifamilyrefinance.com/glossary/building-efficiency-ratio-in-real-estate.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize members\n",
    "        self.resstock_file_name = resstock_file_name\n",
    "        self.resstock_run_version = resstock_run_version\n",
    "        self.year = resstock_year\n",
    "        self.truth_data_version = truth_data_version\n",
    "        self.dataset_name = f'ResStock {self.resstock_run_version} {self.year}'\n",
    "        current_dir = os.getcwd()#os.path.dirname(os.path.abspath(__file__))\n",
    "        self.data_dir = os.path.join(current_dir, '..', 'resstock_data', self.resstock_run_version)\n",
    "        self.truth_data_dir = os.path.join(current_dir, '..', 'truth_data', self.truth_data_version)\n",
    "        self.resource_dir = os.path.join(current_dir, 'resources')\n",
    "        self.output_dir = os.path.join(current_dir, '..', 'output', self.dataset_name)\n",
    "        # self.results_file_name = 'baseline_metadata_and_annual_results.parquet' # self.results_file_name = 'metadata.parquet'\n",
    "        self.results_file_name = 'baseline_metadata_and_annual_results_cluster.csv'\n",
    "        self.egrid_file_name = 'egrid_emissions_2019.csv'\n",
    "        self.downselect_to_multifamily = downselect_to_multifamily\n",
    "        self.multifamily_building_efficiency_ratio = multifamily_building_efficiency_ratio\n",
    "        self.column_definition_file_name = 'resstock_column_definitions.csv'\n",
    "        self.data = None\n",
    "        self.weighted_energy_units = weighted_energy_units\n",
    "        # self.s3_client = boto3.client('s3')\n",
    "        logger.info(f'Creating {self.dataset_name}')\n",
    "\n",
    "        # Make directories\n",
    "        for p in [self.data_dir, self.truth_data_dir, self.output_dir]:\n",
    "            if not os.path.exists(p):\n",
    "                os.makedirs(p)\n",
    "\n",
    "        # S3 location\n",
    "        # self.s3_inpath = f\"s3://{s3_base_dir}/{self.resstock_run_name}\"\n",
    "\n",
    "        # Load and transform data, preserving all columns\n",
    "        self.download_data()\n",
    "        if reload_from_csv:\n",
    "            file_name = f'ResStock wide.csv'\n",
    "            file_path = os.path.join(self.output_dir, file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                 raise FileNotFoundError(\n",
    "                    f'Cannot find {file_path} to reload data, set reload_from_csv=False to create CSV.')\n",
    "            logger.info(f'Reloading from CSV: {file_path}')\n",
    "            self.data = pd.read_csv(file_path)\n",
    "        else:\n",
    "            self.load_data()\n",
    "            self.rename_columns_and_convert_units()\n",
    "            self.add_weighted_area_and_energy_columns()\n",
    "            self.add_dataset_column()\n",
    "            self.add_building_type_group_column()\n",
    "            self.down_to_multifamily()\n",
    "            self.add_multifamily_size_bin_column()\n",
    "            self.reweight_to_multifamily_counts()\n",
    "\n",
    "            logger.debug('ResStock columns after adding all data:')\n",
    "            for c in self.data.columns:\n",
    "                logger.debug(c)\n",
    "\n",
    "    def download_data(self):\n",
    "        # results.csv\n",
    "        results_data_path = os.path.join(self.data_dir, self.results_file_name)\n",
    "        #if not os.path.exists(results_data_path):\n",
    "         #   s3_path = f\"{self.s3_inpath}/metatdata_and_annual_results/national/parquet/Baseline/{self.results_file_name}\" # s3_path = f\"{self.s3_inpath}/metadata/{self.results_file_name}\"\n",
    "          #  data = pd.read_parquet(s3_path, engine=\"pyarrow\")\n",
    "           # data.to_parquet(results_data_path)\n",
    "\n",
    "        # egrid emissions factors\n",
    "        egrid_data_path = os.path.join(self.data_dir, self.egrid_file_name)\n",
    "        #if not os.path.exists(egrid_data_path):\n",
    "         #   s3_file_path = f'truth_data/{self.truth_data_version}/EPA/eGRID/{self.egrid_file_name}'\n",
    "          #  self.read_delimited_truth_data_file_from_S3(s3_file_path, ',')\n",
    "\n",
    "    def load_data(self):\n",
    "        data_file_path = os.path.join(self.data_dir, self.results_file_name)\n",
    "        if not os.path.exists(data_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f'Missing {data_file_path}, cannot load ResStock data')\n",
    "\n",
    "        # self.data = pd.read_parquet(data_file_path)\n",
    "        self.data = pd.read_csv(data_file_path)\n",
    "\n",
    "        logger.debug('ResStock columns before modification:')\n",
    "        for c in self.data.columns:\n",
    "            logger.debug(c)\n",
    "\n",
    "    def rename_columns_and_convert_units(self):\n",
    "        self.data.reset_index(inplace=True)  # bldg_id is the index, make a column\n",
    "\n",
    "        # Add units to all energy columns\n",
    "        energy_cols = [\n",
    "             'out.electricity.range_oven.energy_consumption.kwh',\n",
    "             'out.electricity.plug_loads.energy_consumption.kwh',\n",
    "             'out.natural_gas.total.energy_consumption.kwh',\n",
    "             'out.electricity.hot_tub_pump.energy_consumption.kwh',\n",
    "             'out.electricity.lighting_interior.energy_consumption.kwh',\n",
    "             'out.electricity.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.electricity.heating_hp_bkup_fa.energy_consumption.kwh',\n",
    "             'out.natural_gas.hot_tub_heater.energy_consumption.kwh',\n",
    "             'out.fuel_oil.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.electricity.clothes_washer.energy_consumption.kwh',\n",
    "             'out.site_energy.net.energy_consumption.kwh',\n",
    "             'out.electricity.freezer.energy_consumption.kwh',\n",
    "             'out.electricity.total.energy_consumption.kwh',\n",
    "             'out.electricity.cooling_fans_pumps.energy_consumption.kwh',\n",
    "             'out.natural_gas.lighting.energy_consumption.kwh',\n",
    "             'out.electricity.well_pump.energy_consumption.kwh',\n",
    "             'out.electricity.pool_heater.energy_consumption.kwh',\n",
    "             'out.natural_gas.heating.energy_consumption.kwh',\n",
    "             'out.electricity.lighting_exterior.energy_consumption.kwh',\n",
    "             'out.electricity.ceiling_fan.energy_consumption.kwh',\n",
    "             'out.electricity.lighting_garage.energy_consumption.kwh',\n",
    "             'out.electricity.pv.energy_consumption.kwh',\n",
    "             'out.electricity.hot_water.energy_consumption.kwh',\n",
    "             'out.electricity.cooling.energy_consumption.kwh',\n",
    "             'out.propane.heating.energy_consumption.kwh',\n",
    "             'out.propane.total.energy_consumption.kwh',\n",
    "             'out.electricity.pool_pump.energy_consumption.kwh',\n",
    "             'out.electricity.heating.energy_consumption.kwh',\n",
    "             'out.electricity.heating_fans_pumps.energy_consumption.kwh',\n",
    "             'out.fuel_oil.total.energy_consumption.kwh',\n",
    "             'out.propane.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.natural_gas.grill.energy_consumption.kwh',\n",
    "             'out.electricity.clothes_dryer.energy_consumption.kwh',\n",
    "             'out.fuel_oil.hot_water.energy_consumption.kwh',\n",
    "             'out.electricity.mech_vent.energy_consumption.kwh',\n",
    "             'out.electricity.refrigerator.energy_consumption.kwh',\n",
    "             'out.natural_gas.pool_heater.energy_consumption.kwh',\n",
    "             'out.propane.hot_water.energy_consumption.kwh',\n",
    "             'out.natural_gas.fireplace.energy_consumption.kwh',\n",
    "             'out.natural_gas.range_oven.energy_consumption.kwh',\n",
    "             'out.fuel_oil.heating.energy_consumption.kwh',\n",
    "             'out.electricity.hot_tub_heater.energy_consumption.kwh',\n",
    "             'out.electricity.dishwasher.energy_consumption.kwh',\n",
    "             'out.natural_gas.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.natural_gas.hot_water.energy_consumption.kwh',\n",
    "             'out.electricity.net.energy_consumption.kwh',\n",
    "             'out.propane.clothes_dryer.energy_consumption.kwh',\n",
    "             'out.propane.range_oven.energy_consumption.kwh',\n",
    "             'out.natural_gas.clothes_dryer.energy_consumption.kwh',\n",
    "             'out.site_energy.total.energy_consumption.kwh'\n",
    "        ]\n",
    "\n",
    "        for orig_name in energy_cols:\n",
    "            logger.debug(f\"Processing {orig_name}\")\n",
    "\n",
    "            # Check for unit conversion\n",
    "            orig_units = 'kwh'\n",
    "\n",
    "            # Ensure the column is numeric\n",
    "            self.data[orig_name] = self.data[orig_name].replace('Not Applicable', np.nan)\n",
    "            self.data[orig_name] = self.data[orig_name].astype('float64')\n",
    "\n",
    "            # Append new units to column name, using .. separator for easier parsing\n",
    "            orig_name_short = orig_name[:-4]\n",
    "            new_name = f'{orig_name_short}..{orig_units}'\n",
    "\n",
    "            # Rename the column\n",
    "            logger.debug(f'-- New name = {new_name}')\n",
    "            self.data.rename(columns={orig_name: new_name}, inplace=True)\n",
    "\n",
    "    def down_to_multifamily(self):\n",
    "    # Downselect to just multifamily buildings\n",
    "        if not self.downselect_to_multifamily:\n",
    "            logger.warning('ResStock not downselected to Multifamily: if unintentional, \\\n",
    "                set downselect_to_multifamily=True in constructor')\n",
    "        else:\n",
    "            logger.info('Downselecting ResStock to Multifamily buildings only')\n",
    "            logger.debug(f'before downselect to multifamily, self.data[weight].sum() = {self.data[\"weight\"].sum()}')\n",
    "            self.data = self.data.loc[~(self.data['in.geometry_building_number_units_mf'] == 'None')].copy()\n",
    "            self.data.loc[:, 'in.geometry_building_number_units_mf'] = pd.to_numeric(self.data['in.geometry_building_number_units_mf'])\n",
    "            logger.debug(f'after downselect to multifamily, self.data[weight].sum() = {self.data[\"weight\"].sum()}')\n",
    "\n",
    "    def add_multifamily_size_bin_column(self):\n",
    "    # Adds bins for the size of the multifamily buildings the unit is inside\n",
    "        if not self.downselect_to_multifamily:\n",
    "            return\n",
    "\n",
    "        # Estimate the rentable floor area of the building this unit is in\n",
    "        self.data.loc[:, 'in.rentable_floor_area_of_building_this_unit_is_in..ft2'] = self.data['in.geometry_building_number_units_mf'] * self.data[self.FLR_AREA]\n",
    "\n",
    "        # Estimate the total floor area of the building the unit is in, including common areas\n",
    "        ber = self.multifamily_building_efficiency_ratio\n",
    "        logger.info(f'Assuming a rentable area to gross area ratio of {ber:.2f} when setting multifamily building size bins only.')\n",
    "        logger.info(f'This is reflected ONLY in \"{self.FLR_AREA_CAT}\" and \"in.total_floor_area_of_building_this_unit_is_in..ft2\"')\n",
    "        logger.info(f'It is not reflected in the weighted energy or floor area columns!')\n",
    "        self.data.loc[:, 'in.total_floor_area_of_building_this_unit_is_in..ft2'] = self.data['in.geometry_building_number_units_mf'] * self.data[self.FLR_AREA] / ber\n",
    "\n",
    "        # Put each model into a bin by floor area of the building the unit is inside\n",
    "        def size_bin(row):\n",
    "            sf = row['in.total_floor_area_of_building_this_unit_is_in..ft2']\n",
    "\n",
    "            # Bin the square footage\n",
    "            if sf < 1_000:\n",
    "                b = '_1000'\n",
    "            elif sf < 5_000:\n",
    "                b = '1001_5000'\n",
    "            elif sf < 10_000:\n",
    "                b = '5001_10000'\n",
    "            elif sf < 25_000:\n",
    "                b = '10001_25000'\n",
    "            elif sf < 50_000:\n",
    "                b = '25001_50000'\n",
    "            elif sf < 100_000:\n",
    "                b = '50001_100000'\n",
    "            elif sf < 200_000:\n",
    "                b = '100001_200000'\n",
    "            elif sf < 500_000:\n",
    "                b = '200001_500000'\n",
    "            elif sf < 1_000_000:\n",
    "                b = '500001_1mil'\n",
    "            else:\n",
    "                b = 'over_1mil'\n",
    "\n",
    "            return b\n",
    "\n",
    "        self.data.loc[:, self.FLR_AREA_CAT] = self.data.apply(lambda row: size_bin(row), axis=1)\n",
    "\n",
    "    def reweight_to_multifamily_counts(self):\n",
    "    # Changes the weights from the ResStock convention of representing number of units\n",
    "    # to the ComStock convention of representing number of buildings.\n",
    "    # Weights will be fractional as each unit is a fraction of a single building.\n",
    "        if not self.downselect_to_multifamily:\n",
    "            return\n",
    "        else:\n",
    "            logger.info('Recalculating weights to represent number of multifamily buildings represented')\n",
    "\n",
    "        # Reweight to approximate number of multifamily buildings of the given size represented by\n",
    "        # the results for this model.\n",
    "        def reweight_to_bldg_count(row):\n",
    "            sf_of_building_unit_is_in = row['in.rentable_floor_area_of_building_this_unit_is_in..ft2']\n",
    "            sf_represented_by_results = row[self.FLR_AREA] * row['weight']\n",
    "            num_bldgs_represented_by_results = sf_represented_by_results / sf_of_building_unit_is_in\n",
    "\n",
    "            return num_bldgs_represented_by_results\n",
    "\n",
    "        self.data.loc[:, self.BLDG_WEIGHT] = self.data.apply(lambda row: reweight_to_bldg_count(row), axis=1)\n",
    "\n",
    "    def add_dataset_column(self):\n",
    "        self.data.loc[:, 'dataset'] = self.dataset_name\n",
    "\n",
    "    def add_building_type_group_column(self):\n",
    "        self.data.loc[:, 'in.comstock_building_type_group'] = 'Multifamily'\n",
    "\n",
    "    def add_weighted_area_and_energy_columns(self):\n",
    "        # Area - ensure the column is numeric then create weighted column\n",
    "        self.data.loc[:, self.FLR_AREA] = self.data[self.FLR_AREA].replace('Not Applicable', np.nan)\n",
    "        self.data.loc[:, self.FLR_AREA] = self.data[self.FLR_AREA].astype('float64')\n",
    "        new_area_col = self.col_name_to_weighted(self.FLR_AREA)\n",
    "        self.data.loc[:, new_area_col] = self.data[self.FLR_AREA] * self.data[self.BLDG_WEIGHT]\n",
    "\n",
    "        # Energy\n",
    "        for col in (self.COLS_TOT_ANN_ENGY + self.COLS_ENDUSE_ANN_ENGY):\n",
    "            # Ensure the energy column is numeric then create weighted column\n",
    "            self.data.loc[:, col] = self.data[col].replace('Not Applicable', np.nan).astype('float64')\n",
    "            new_col = self.col_name_to_weighted(col, self.weighted_energy_units)\n",
    "\n",
    "            # Weight and convert to TBtu\n",
    "            old_units = self.units_from_col_name(col)\n",
    "            new_units = self.weighted_energy_units\n",
    "            conv_fact = self.conv_fact(old_units, new_units)\n",
    "            self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
    "\n",
    "    def export_to_csv_wide(self):\n",
    "        # Exports resstock data to CSV in wide format\n",
    "\n",
    "        file_name = f'ResStock wide.csv'\n",
    "        file_path = os.path.join(self.output_dir, file_name)\n",
    "        self.data.to_csv(file_path, index=False)\n",
    "\n",
    "    def export_to_csv_long(self, add_egrid_emissions=True):\n",
    "        # Exports resstock data to CSV in long format, with rows for each end use\n",
    "\n",
    "        # Convert ResStock into long format, with a new row for each Fuel.Enduse combination\n",
    "        engy_cols = []\n",
    "        for col in (self.COLS_ENDUSE_ANN_ENGY):\n",
    "            engy_cols.append(self.col_name_to_weighted(col, self.weighted_energy_units))\n",
    "\n",
    "        bldg_cols = []\n",
    "        for c in self.data.columns:\n",
    "            if not 'out.' in c:\n",
    "                bldg_cols.append(c)\n",
    "\n",
    "        var_col = 'type.fuel.enduse.energy_consumption..units'\n",
    "        val_col = 'weighted_energy_consumption'\n",
    "        dl = pd.melt(self.data, id_vars=[self.BLDG_ID, 'in.state'], value_vars=engy_cols, var_name=var_col, value_name=val_col)\n",
    "\n",
    "        # Remove rows with zero values for the fuel type/end use combo\n",
    "        dl = dl.loc[dl[val_col] > 0]\n",
    "\n",
    "        # Sort by building ID\n",
    "        dl.sort_values(self.BLDG_ID, inplace=True)\n",
    "\n",
    "        # Separate'type.fuel.enduse.energy_consumption..units' into multiple columns\n",
    "        def split_col_name(col_name):\n",
    "            p = self.engy_col_name_to_parts(col_name)\n",
    "\n",
    "            return [p['fuel'], p['enduse'], p['units'], ]\n",
    "\n",
    "        dl['fuel'], dl['enduse'], dl['weighted_energy_consumption_units'] = zip(*dl[var_col].apply(split_col_name))\n",
    "\n",
    "        # Drop the combined type.fuel.enduse.energy_consumption..units column\n",
    "        dl.drop(var_col, axis=1, inplace=True)\n",
    "\n",
    "        if add_egrid_emissions:\n",
    "            logger.info('Adding emissions using eGRID 2019 emissions factors')\n",
    "            # Read the emissions data\n",
    "            file_name = self.egrid_file_name\n",
    "            file_path = os.path.join(self.data_dir, file_name)\n",
    "            egrid = pd.read_csv(file_path, index_col='State')\n",
    "\n",
    "            # metric_ton_to_lb = 2204.62\n",
    "            cf = (1.0/1e3)*(1.0/3.412)*(1e9/1.0)*(1.0/2204.62)*(1.0/1e6)\n",
    "\n",
    "            egrid['million_metric_ton_CO2e_per_TBtu'] = egrid['total_output_emissions_rates_CO2e_lb_per_MWh'] * cf\n",
    "\n",
    "            # Calculate the emissions factors for each row\n",
    "            def emissions(egrid, row):\n",
    "                # Determine emissions factor by fuel (and location, for electricity)\n",
    "                fuel = row['fuel']\n",
    "                state = row['in.state']\n",
    "                if pd.isna(state):\n",
    "                    logger.error(f'Missing electric emissions factor for state {row[\"in.state\"]}')\n",
    "                    return 0.0\n",
    "                if fuel == 'electricity':\n",
    "                    cf_million_metric_tons_per_TBtu = egrid.loc[state]['million_metric_ton_CO2e_per_TBtu']\n",
    "                elif fuel == 'natural_gas':\n",
    "                    # Natural Gas for homes and businesses: 116.65 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    # Plus 2.3% leakage rate of methane calculated from Science paper\n",
    "                    # https://www.science.org/doi/10.1126/science.aar7204\n",
    "                    cf_million_metric_tons_per_TBtu = 141.67/1000000*1000*1E9/2204.62/1000000\n",
    "                elif fuel == 'fuel_oil':\n",
    "                    # Home Heating Fuel for homes and businesses: 163.45 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    cf_million_metric_tons_per_TBtu = 163.45/1000000*1000*1E9/2204.62/1000000\n",
    "                elif fuel == 'propane':\n",
    "                    # Propane for homes and businesses: 138.63 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    cf_million_metric_tons_per_TBtu = 138.63/1000000*1000*1E9/2204.62/1000000\n",
    "                #elif fuel == 'wood':\n",
    "                    # No data for wood or biomass here\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    # return 0.0\n",
    "                else:\n",
    "                    raise Exception(f'Fuel type {fuel} was not recognized, cannot calculate emissions factor')\n",
    "\n",
    "                # Calculate emssions (million metric tons)\n",
    "                energy_units = row['weighted_energy_consumption_units']\n",
    "                print(energy_units)\n",
    "                assert(energy_units == 'tbtu')\n",
    "                energy_TBtu = row['weighted_energy_consumption']\n",
    "                emis_mmt = energy_TBtu * cf_million_metric_tons_per_TBtu\n",
    "\n",
    "                return emis_mmt\n",
    "\n",
    "            dl['ghg_emissions..million_metric_tons_CO2e)'] = dl.apply(lambda row: emissions(egrid, row), axis=1)\n",
    "\n",
    "            # Drop the in.state_abbreviation column, will be in building characteristics data\n",
    "            dl.drop('in.state', axis=1, inplace=True)\n",
    "\n",
    "        # Save files - separate building energy from characteristics for file size\n",
    "        file_name = f'ResStock energy long.csv'\n",
    "        file_path = os.path.join(self.output_dir, file_name)\n",
    "        print(file_path)\n",
    "        dl.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeabe67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating ResStock 2024.1 TMY3\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_12490/2781291742.py:98: DtypeWarning: Columns (131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(data_file_path)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_12490/2781291742.py:279: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_12490/2781291742.py:279: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_12490/2781291742.py:279: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_12490/2781291742.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, 'dataset'] = self.dataset_name\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_12490/2781291742.py:260: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, 'in.comstock_building_type_group'] = 'Multifamily'\n",
      "INFO:__main__:Downselecting ResStock to Multifamily buildings only\n",
      "INFO:__main__:Assuming a rentable area to gross area ratio of 0.75 when setting multifamily building size bins only.\n",
      "INFO:__main__:This is reflected ONLY in \"in.floor_area_category\" and \"in.total_floor_area_of_building_this_unit_is_in..ft2\"\n",
      "INFO:__main__:It is not reflected in the weighted energy or floor area columns!\n",
      "INFO:__main__:Recalculating weights to represent number of multifamily buildings represented\n"
     ]
    }
   ],
   "source": [
    "df = ResStock_2(\"baseline_and_annual_results.parquet\", \"2024.1\", \"TMY3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316f41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.export_to_csv_wide()\n",
    "#df.export_to_csv_long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe81e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Adding emissions using eGRID 2019 emissions factors\n"
     ]
    }
   ],
   "source": [
    "df.export_to_csv_long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e127f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
