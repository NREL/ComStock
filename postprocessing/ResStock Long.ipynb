{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988acccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textwrap import indent\n",
    "\n",
    "import boto3\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from comstockpostproc.resstock_naming_mixin_LARGEE import ResStockNamingMixin\n",
    "from comstockpostproc.units_mixin import UnitsMixin\n",
    "from comstockpostproc.s3_utilities_mixin import S3UtilitiesMixin\n",
    "from comstockpostproc import resstock_LARGEE\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf6e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResStock_2(ResStockNamingMixin, UnitsMixin, S3UtilitiesMixin):\n",
    "    def __init__(self, resstock_file_name, resstock_run_version = '2024.1', resstock_year = 'TMY3', truth_data_version='v01',\n",
    "        weighted_energy_units='tbtu', reload_from_csv=False, downselect_to_multifamily=True,\n",
    "        multifamily_building_efficiency_ratio=0.75):\n",
    "        \"\"\"\n",
    "        A class to load and transform ResStock multifamily data for export, analysis, and comparison.\n",
    "        Set up to pull from postprocessed results from S3 OEDI bucket, not raw ResStock results from S3 RESBLDG.\n",
    "        Args:\n",
    "\n",
    "            resstock_run_s3_dir (str): The location of the ResStock run on S3 in the OEDI bucket\n",
    "            resstock_run_name (str): The name of the ResStock run, used to look\n",
    "            up the data on S3\n",
    "            resstock_year (int): The year represented by this ResStock run\n",
    "            resstock_run_version (str): The version string for this ResStock run\n",
    "            to differentiate it from other ResStock runs\n",
    "            downselect_to_multifamily (bool): If True, drops all non-multifamily results\n",
    "            multifamily_building_efficiency_ratio (float): 0-1 fraction of rentable area (units) to gross area,\n",
    "            which includes common areas such as corridors, lobbies, gyms, etc.\n",
    "            Typical ratio is 75% for multfamily per several sources such as\n",
    "            https://multifamilyrefinance.com/glossary/building-efficiency-ratio-in-real-estate.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize members\n",
    "        self.resstock_file_name = resstock_file_name\n",
    "        self.resstock_run_version = resstock_run_version\n",
    "        self.year = resstock_year\n",
    "        self.truth_data_version = truth_data_version\n",
    "        self.dataset_name = f'ResStock {self.resstock_run_version} {self.year}'\n",
    "        current_dir = os.getcwd()#os.path.dirname(os.path.abspath(__file__))\n",
    "        self.data_dir = os.path.join(current_dir, '..', 'resstock_data', self.resstock_run_version)\n",
    "        self.truth_data_dir = os.path.join(current_dir, '..', 'truth_data', self.truth_data_version)\n",
    "        self.resource_dir = os.path.join(current_dir, 'resources')\n",
    "        self.output_dir = os.path.join(current_dir, '..', 'output', self.dataset_name)\n",
    "        # self.results_file_name = 'baseline_metadata_and_annual_results.parquet' # self.results_file_name = 'metadata.parquet'\n",
    "        self.results_file_name = 'baseline_metadata_and_annual_results_cluster.csv'\n",
    "        self.egrid_file_name = 'egrid_state_emissions_factors.csv' #need to downselect to 2021\n",
    "        self.downselect_to_multifamily = downselect_to_multifamily\n",
    "        self.multifamily_building_efficiency_ratio = multifamily_building_efficiency_ratio\n",
    "        self.column_definition_file_name = 'resstock_column_definitions.csv'\n",
    "        self.data = None\n",
    "        self.weighted_energy_units = weighted_energy_units\n",
    "        # self.s3_client = boto3.client('s3')\n",
    "        logger.info(f'Creating {self.dataset_name}')\n",
    "\n",
    "        # Make directories\n",
    "        for p in [self.data_dir, self.truth_data_dir, self.output_dir]:\n",
    "            if not os.path.exists(p):\n",
    "                os.makedirs(p)\n",
    "\n",
    "        # S3 location\n",
    "        # self.s3_inpath = f\"s3://{s3_base_dir}/{self.resstock_run_name}\"\n",
    "\n",
    "        # Load and transform data, preserving all columns\n",
    "        self.download_data()\n",
    "        if reload_from_csv:\n",
    "            file_name = f'ResStock wide.csv'\n",
    "            file_path = os.path.join(self.output_dir, file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                 raise FileNotFoundError(\n",
    "                    f'Cannot find {file_path} to reload data, set reload_from_csv=False to create CSV.')\n",
    "            logger.info(f'Reloading from CSV: {file_path}')\n",
    "            self.data = pd.read_csv(file_path)\n",
    "        else:\n",
    "            self.load_data()\n",
    "            self.rename_columns_and_convert_units()\n",
    "            self.add_weighted_area_and_energy_columns()\n",
    "            self.add_dataset_column()\n",
    "            self.add_building_type_group_column()\n",
    "            # self.down_to_multifamily()\n",
    "            self.add_multifamily_size_bin_column()\n",
    "            self.reweight_to_multifamily_counts()\n",
    "\n",
    "            logger.debug('ResStock columns after adding all data:')\n",
    "            for c in self.data.columns:\n",
    "                logger.debug(c)\n",
    "\n",
    "    def download_data(self):\n",
    "        # results.csv\n",
    "        results_data_path = os.path.join(self.data_dir, self.results_file_name)\n",
    "        #if not os.path.exists(results_data_path):\n",
    "         #   s3_path = f\"{self.s3_inpath}/metatdata_and_annual_results/national/parquet/Baseline/{self.results_file_name}\" # s3_path = f\"{self.s3_inpath}/metadata/{self.results_file_name}\"\n",
    "          #  data = pd.read_parquet(s3_path, engine=\"pyarrow\")\n",
    "           # data.to_parquet(results_data_path)\n",
    "\n",
    "        # egrid emissions factors\n",
    "        self.egrid_data_path = os.path.join(self.data_dir, self.egrid_file_name)\n",
    "        #if not os.path.exists(egrid_data_path):\n",
    "         #   s3_file_path = f'truth_data/{self.truth_data_version}/EPA/eGRID/{self.egrid_file_name}'\n",
    "          #  self.read_delimited_truth_data_file_from_S3(s3_file_path, ',')\n",
    "\n",
    "    def load_data(self):\n",
    "        data_file_path = os.path.join(self.data_dir, self.results_file_name)\n",
    "        if not os.path.exists(data_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f'Missing {data_file_path}, cannot load ResStock data')\n",
    "\n",
    "        # self.data = pd.read_parquet(data_file_path)\n",
    "        self.data = pd.read_csv(data_file_path)\n",
    "\n",
    "        logger.debug('ResStock columns before modification:')\n",
    "        for c in self.data.columns:\n",
    "            logger.debug(c)\n",
    "\n",
    "    def rename_columns_and_convert_units(self):\n",
    "        self.data.reset_index(inplace=True)  # bldg_id is the index, make a column\n",
    "\n",
    "        # Add units to all energy columns\n",
    "        energy_cols = [ #self.COLS_ENDUSE_ANN_ENGY\n",
    "             'out.electricity.range_oven.energy_consumption.kwh',\n",
    "             'out.electricity.plug_loads.energy_consumption.kwh',\n",
    "             'out.natural_gas.total.energy_consumption.kwh',\n",
    "             'out.electricity.hot_tub_pump.energy_consumption.kwh',\n",
    "             'out.electricity.lighting_interior.energy_consumption.kwh',\n",
    "             'out.electricity.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.electricity.heating_hp_bkup_fa.energy_consumption.kwh',\n",
    "             'out.natural_gas.hot_tub_heater.energy_consumption.kwh',\n",
    "             'out.fuel_oil.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.electricity.clothes_washer.energy_consumption.kwh',\n",
    "             'out.site_energy.net.energy_consumption.kwh',\n",
    "             'out.electricity.freezer.energy_consumption.kwh',\n",
    "             'out.electricity.total.energy_consumption.kwh',\n",
    "             'out.electricity.cooling_fans_pumps.energy_consumption.kwh',\n",
    "             'out.natural_gas.lighting.energy_consumption.kwh',\n",
    "             'out.electricity.well_pump.energy_consumption.kwh',\n",
    "             'out.electricity.pool_heater.energy_consumption.kwh',\n",
    "             'out.natural_gas.heating.energy_consumption.kwh',\n",
    "             'out.electricity.lighting_exterior.energy_consumption.kwh',\n",
    "             'out.electricity.ceiling_fan.energy_consumption.kwh',\n",
    "             'out.electricity.lighting_garage.energy_consumption.kwh',\n",
    "             'out.electricity.pv.energy_consumption.kwh',\n",
    "             'out.electricity.hot_water.energy_consumption.kwh',\n",
    "             'out.electricity.cooling.energy_consumption.kwh',\n",
    "             'out.propane.heating.energy_consumption.kwh',\n",
    "             'out.propane.total.energy_consumption.kwh',\n",
    "             'out.electricity.pool_pump.energy_consumption.kwh',\n",
    "             'out.electricity.heating.energy_consumption.kwh',\n",
    "             'out.electricity.heating_fans_pumps.energy_consumption.kwh',\n",
    "             'out.fuel_oil.total.energy_consumption.kwh',\n",
    "             'out.propane.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.natural_gas.grill.energy_consumption.kwh',\n",
    "             'out.electricity.clothes_dryer.energy_consumption.kwh',\n",
    "             'out.fuel_oil.hot_water.energy_consumption.kwh',\n",
    "             'out.electricity.mech_vent.energy_consumption.kwh',\n",
    "             'out.electricity.refrigerator.energy_consumption.kwh',\n",
    "             'out.natural_gas.pool_heater.energy_consumption.kwh',\n",
    "             'out.propane.hot_water.energy_consumption.kwh',\n",
    "             'out.natural_gas.fireplace.energy_consumption.kwh',\n",
    "             'out.natural_gas.range_oven.energy_consumption.kwh',\n",
    "             'out.fuel_oil.heating.energy_consumption.kwh',\n",
    "             'out.electricity.hot_tub_heater.energy_consumption.kwh',\n",
    "             'out.electricity.dishwasher.energy_consumption.kwh',\n",
    "             'out.natural_gas.heating_hp_bkup.energy_consumption.kwh',\n",
    "             'out.natural_gas.hot_water.energy_consumption.kwh',\n",
    "             'out.electricity.net.energy_consumption.kwh',\n",
    "             'out.propane.clothes_dryer.energy_consumption.kwh',\n",
    "             'out.propane.range_oven.energy_consumption.kwh',\n",
    "             'out.natural_gas.clothes_dryer.energy_consumption.kwh',\n",
    "             'out.site_energy.total.energy_consumption.kwh'\n",
    "        ]\n",
    "\n",
    "        for orig_name in energy_cols:\n",
    "            logger.debug(f\"Processing {orig_name}\")\n",
    "\n",
    "            # Check for unit conversion\n",
    "            orig_units = 'kwh'\n",
    "\n",
    "            # Ensure the column is numeric\n",
    "            self.data[orig_name] = self.data[orig_name].replace('Not Applicable', np.nan)\n",
    "            self.data[orig_name] = self.data[orig_name].astype('float64')\n",
    "\n",
    "            # Append new units to column name, using .. separator for easier parsing\n",
    "            orig_name_short = orig_name[:-4]\n",
    "            new_name = f'{orig_name_short}..{orig_units}'\n",
    "\n",
    "            # Rename the column\n",
    "            logger.debug(f'-- New name = {new_name}')\n",
    "            self.data.rename(columns={orig_name: new_name}, inplace=True)\n",
    "\n",
    "    def down_to_multifamily(self):\n",
    "    # Downselect to just multifamily buildings\n",
    "        if not self.downselect_to_multifamily:\n",
    "            logger.warning('ResStock not downselected to Multifamily: if unintentional, \\\n",
    "                set downselect_to_multifamily=True in constructor')\n",
    "        else:\n",
    "            logger.info('Downselecting ResStock to Multifamily buildings only')\n",
    "            logger.debug(f'before downselect to multifamily, self.data[weight].sum() = {self.data[\"weight\"].sum()}')\n",
    "            self.data = self.data.loc[~(self.data['in.geometry_building_number_units_mf'] == 'None')].copy()\n",
    "            self.data.loc[:, 'in.geometry_building_number_units_mf'] = pd.to_numeric(self.data['in.geometry_building_number_units_mf'])\n",
    "            logger.debug(f'after downselect to multifamily, self.data[weight].sum() = {self.data[\"weight\"].sum()}')\n",
    "\n",
    "    def add_multifamily_size_bin_column(self):\n",
    "    # Adds bins for the size of the multifamily buildings the unit is inside\n",
    "        if not self.downselect_to_multifamily:\n",
    "            return\n",
    "\n",
    "        # Estimate the rentable floor area of the building this unit is in\n",
    "        self.data.loc[:, 'in.rentable_floor_area_of_building_this_unit_is_in..ft2'] = self.data['in.geometry_building_number_units_mf'] * self.data[self.FLR_AREA]\n",
    "\n",
    "        # Estimate the total floor area of the building the unit is in, including common areas\n",
    "        ber = self.multifamily_building_efficiency_ratio\n",
    "        logger.info(f'Assuming a rentable area to gross area ratio of {ber:.2f} when setting multifamily building size bins only.')\n",
    "        logger.info(f'This is reflected ONLY in \"{self.FLR_AREA_CAT}\" and \"in.total_floor_area_of_building_this_unit_is_in..ft2\"')\n",
    "        logger.info(f'It is not reflected in the weighted energy or floor area columns!')\n",
    "        self.data.loc[:, 'in.total_floor_area_of_building_this_unit_is_in..ft2'] = self.data['in.geometry_building_number_units_mf'] * self.data[self.FLR_AREA] / ber\n",
    "\n",
    "        # Put each model into a bin by floor area of the building the unit is inside\n",
    "        def size_bin(row):\n",
    "            sf = row['in.total_floor_area_of_building_this_unit_is_in..ft2']\n",
    "\n",
    "            # Bin the square footage\n",
    "            if sf < 1_000:\n",
    "                b = '_1000'\n",
    "            elif sf < 5_000:\n",
    "                b = '1001_5000'\n",
    "            elif sf < 10_000:\n",
    "                b = '5001_10000'\n",
    "            elif sf < 25_000:\n",
    "                b = '10001_25000'\n",
    "            elif sf < 50_000:\n",
    "                b = '25001_50000'\n",
    "            elif sf < 100_000:\n",
    "                b = '50001_100000'\n",
    "            elif sf < 200_000:\n",
    "                b = '100001_200000'\n",
    "            elif sf < 500_000:\n",
    "                b = '200001_500000'\n",
    "            elif sf < 1_000_000:\n",
    "                b = '500001_1mil'\n",
    "            else:\n",
    "                b = 'over_1mil'\n",
    "\n",
    "            return b\n",
    "\n",
    "        self.data.loc[:, self.FLR_AREA_CAT] = self.data.apply(lambda row: size_bin(row), axis=1)\n",
    "        self.data['bps_tag'] = False\n",
    "        self.data.loc[(self.data['in.total_floor_area_of_building_this_unit_is_in..ft2']>= 10000) & (self.data['in.geometry_building_type_recs']== 'Multi-Family with 5+ Units'), 'bps_tag'] = True\n",
    "\n",
    "    def reweight_to_multifamily_counts(self):\n",
    "    # Changes the weights from the ResStock convention of representing number of units\n",
    "    # to the ComStock convention of representing number of buildings.\n",
    "    # Weights will be fractional as each unit is a fraction of a single building.\n",
    "        if not self.downselect_to_multifamily:\n",
    "            return\n",
    "        else:\n",
    "            logger.info('Recalculating weights to represent number of multifamily buildings represented')\n",
    "\n",
    "        # Reweight to approximate number of multifamily buildings of the given size represented by\n",
    "        # the results for this model.\n",
    "        def reweight_to_bldg_count(row):\n",
    "            sf_of_building_unit_is_in = row['in.rentable_floor_area_of_building_this_unit_is_in..ft2']\n",
    "            sf_represented_by_results = row[self.FLR_AREA] * row['weight']\n",
    "            num_bldgs_represented_by_results = sf_represented_by_results / sf_of_building_unit_is_in\n",
    "\n",
    "            return num_bldgs_represented_by_results\n",
    "\n",
    "        self.data.loc[:,'units_represented'] = self.data.loc[:,'weight'].copy()\n",
    "        self.data.loc[:, \"bldg_represented\"] = self.data.apply(lambda row: reweight_to_bldg_count(row), axis=1)\n",
    "\n",
    "    def add_dataset_column(self):\n",
    "        self.data.loc[:, 'dataset'] = self.dataset_name\n",
    "\n",
    "    def add_building_type_group_column(self):\n",
    "        self.data.loc[:, 'in.comstock_building_type_group'] = 'Multifamily'\n",
    "        \n",
    "\n",
    "    def add_weighted_area_and_energy_columns(self):\n",
    "        # Area - ensure the column is numeric then create weighted column\n",
    "        self.data.loc[:, self.FLR_AREA] = self.data[self.FLR_AREA].replace('Not Applicable', np.nan)\n",
    "        self.data.loc[:, self.FLR_AREA] = self.data[self.FLR_AREA].astype('float64')\n",
    "        new_area_col = self.col_name_to_weighted(self.FLR_AREA)\n",
    "        self.data.loc[:, new_area_col] = self.data[self.FLR_AREA] * self.data[self.BLDG_WEIGHT]\n",
    "\n",
    "        # file_name = self.egrid_file_name\n",
    "        # file_path = os.path.join(self.data_dir, file_name)\n",
    "        egrid = pd.read_csv(self.egrid_data_path, index_col='state')\n",
    "\n",
    "        # metric_ton_to_lb = 2204.62\n",
    "        cf = (1.0/1e3)*(1.0/3.412)*(1e9/1.0)*(1.0/2204.62)*(1.0/1e6)\n",
    "\n",
    "        egrid['million_metric_ton_CO2e_per_TBtu'] = egrid['2021'] * cf\n",
    "        egrid_dct = dict(zip(egrid.index,egrid['million_metric_ton_CO2e_per_TBtu']))\n",
    "        cf_million_metric_tons_per_TBtu = self.data['in.state'].map(egrid_dct)\n",
    "\n",
    "        # Energy & emissions\n",
    "\n",
    "        def energy_emissions_by_enduse_group(enduse_list, group_name):\n",
    "            emissions_col_list = []\n",
    "            fuels = []\n",
    "            for col in enduse_list:\n",
    "                self.data.loc[:, col] = self.data[col].replace('Not Applicable', np.nan).astype('float64')\n",
    "                new_col = self.col_name_to_weighted(col, self.weighted_energy_units)\n",
    "\n",
    "                # Weight and convert to TBtu\n",
    "                old_units = self.units_from_col_name(col)\n",
    "                new_units = self.weighted_energy_units\n",
    "                conv_fact = self.conv_fact(old_units, new_units)\n",
    "                self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
    "\n",
    "                # Create a corresponding emissions column\n",
    "                emission_col = ('.').join(new_col.split('.')[0:4]) + '.emissions..co2e_mmt'\n",
    "                emissions_col_list.append(emission_col)\n",
    "                \n",
    "                if 'electricity' in new_col:\n",
    "                    self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
    "                    fuels.append('electricity')\n",
    "                elif 'natural_gas' in new_col:\n",
    "                    # Natural Gas for homes and businesses: 116.65 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    # Plus 2.3% leakage rate of methane calculated from Science paper\n",
    "                    # https://www.science.org/doi/10.1126/science.aar7204\n",
    "                    self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
    "                    fuels.append('natural_gas')\n",
    "                elif 'propane' in new_col:\n",
    "                    # Propane for homes and businesses: 138.63 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    self.data.loc[:, emission_col] = self.data[new_col] * 138.63/1000000*1000*1E9/2204.62/1000000\n",
    "                    fuels.append('propane')\n",
    "                else:\n",
    "                    # Home Heating Fuel for homes and businesses: 163.45 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    self.data.loc[:, emission_col] = self.data[new_col] * 163.45/1000000*1000*1E9/2204.62/1000000\n",
    "                    fuels.append('fuel_oil')\n",
    "                    \n",
    "                \n",
    "            #Aggregate total emissions by enduse and fuel\n",
    "            for fuel in fuels:\n",
    "                x = [i for i in emissions_col_list if fuel in i]\n",
    "                fuel_group_name = 'out.weighted.' + fuel + '.group.' + group_name + '.emissions..co2e_mmt'\n",
    "                self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
    "            total_name = 'out.weighted.total.group.'+group_name+'.emissions..co2e_mmt' \n",
    "            self.data.loc[:, total_name ] = self.data[emissions_col_list].sum(axis=1)\n",
    "\n",
    "        energy_emissions_by_enduse_group(self.COLS_HVAC_ENDUSE, 'hvac')\n",
    "        energy_emissions_by_enduse_group(self.COLS_DHW_ENDUSE, 'hot_water')\n",
    "        energy_emissions_by_enduse_group(self.COLS_APP_ENDUSE, 'appliance')\n",
    "        energy_emissions_by_enduse_group(self.COLS_LIGHT_ENDUSE, 'lighting')\n",
    "        energy_emissions_by_enduse_group(self.COLS_MISC_ENDUSE, 'miscellaneous')\n",
    "        energy_emissions_by_enduse_group(self.COLS_PV_ENDUSE, 'pv')\n",
    "\n",
    "    def export_to_csv_wide(self):\n",
    "        # Exports resstock data to CSV in wide format\n",
    "\n",
    "        file_name = f'ResStock wide.csv'\n",
    "        file_path = os.path.join(self.output_dir, file_name)\n",
    "        self.data.to_csv(file_path, index=False)\n",
    "\n",
    "    def export_to_csv_long(self, add_egrid_emissions=True):\n",
    "        # Exports resstock data to CSV in long format, with rows for each end use\n",
    "\n",
    "        # Convert ResStock into long format, with a new row for each Fuel.Enduse combination\n",
    "        engy_cols = []\n",
    "        for col in (self.COLS_ENDUSE_ANN_ENGY):\n",
    "            engy_cols.append(self.col_name_to_weighted(col, self.weighted_energy_units))\n",
    "\n",
    "        #Grab corresponding emissions columns\n",
    "\n",
    "        bldg_cols = []\n",
    "        for c in self.data.columns:\n",
    "            if not 'out.' in c:\n",
    "                bldg_cols.append(c)\n",
    "\n",
    "        var_col = 'type.fuel.enduse.energy_consumption..units'\n",
    "        val_col = 'weighted_energy_consumption'\n",
    "        # Add in emissions to pd.melt, remove state? \n",
    "        dl = pd.melt(self.data, id_vars=[self.BLDG_ID, 'in.state'], value_vars=engy_cols, var_name=var_col, value_name=val_col)\n",
    "\n",
    "        # Remove rows with zero values for the fuel type/end use combo\n",
    "        dl = dl.loc[dl[val_col] > 0]\n",
    "\n",
    "        # Sort by building ID\n",
    "        dl.sort_values(self.BLDG_ID, inplace=True)\n",
    "\n",
    "        # Separate'type.fuel.enduse.energy_consumption..units' into multiple columns\n",
    "        def split_col_name(col_name):\n",
    "            p = self.engy_col_name_to_parts(col_name)\n",
    "\n",
    "            return [p['fuel'], p['enduse'], p['units'], ]\n",
    "\n",
    "        dl['fuel'], dl['enduse'], dl['weighted_energy_consumption_units'] = zip(*dl[var_col].apply(split_col_name))\n",
    "\n",
    "        # Pull in enduse group here\n",
    "    \n",
    "\n",
    "        # Drop the combined type.fuel.enduse.energy_consumption..units column\n",
    "        dl.drop(var_col, axis=1, inplace=True)\n",
    "        \n",
    "        if add_egrid_emissions:\n",
    "            logger.info('Adding emissions using eGRID 2019 emissions factors')\n",
    "            # Read the emissions data\n",
    "            file_name = self.egrid_file_name\n",
    "            file_path = os.path.join(self.data_dir, file_name)\n",
    "            egrid = pd.read_csv(file_path, index_col='state')\n",
    "\n",
    "            # metric_ton_to_lb = 2204.62\n",
    "            cf = (1.0/1e3)*(1.0/3.412)*(1e9/1.0)*(1.0/2204.62)*(1.0/1e6)\n",
    "\n",
    "            #egrid['million_metric_ton_CO2e_per_TBtu'] = egrid['total_output_emissions_rates_CO2e_lb_per_MWh'] * cf\n",
    "            egrid['million_metric_ton_CO2e_per_TBtu'] = egrid['2021'] * cf\n",
    "\n",
    "            # Calculate the emissions factors for each row\n",
    "            def emissions(egrid, row):\n",
    "                # Determine emissions factor by fuel (and location, for electricity)\n",
    "                fuel = row['fuel']\n",
    "                state = row['in.state']\n",
    "                if pd.isna(state):\n",
    "                    logger.error(f'Missing electric emissions factor for state {row[\"in.state\"]}')\n",
    "                    return 0.0\n",
    "                if fuel == 'electricity':\n",
    "                    cf_million_metric_tons_per_TBtu = egrid.loc[state]['million_metric_ton_CO2e_per_TBtu']\n",
    "                elif fuel == 'natural_gas':\n",
    "                    # Natural Gas for homes and businesses: 116.65 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    # Plus 2.3% leakage rate of methane calculated from Science paper\n",
    "                    # https://www.science.org/doi/10.1126/science.aar7204\n",
    "                    cf_million_metric_tons_per_TBtu = 141.67/1000000*1000*1E9/2204.62/1000000\n",
    "                elif fuel == 'fuel_oil':\n",
    "                    # Home Heating Fuel for homes and businesses: 163.45 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    cf_million_metric_tons_per_TBtu = 163.45/1000000*1000*1E9/2204.62/1000000\n",
    "                elif fuel == 'propane':\n",
    "                    # Propane for homes and businesses: 138.63 lb CO2/million BTU\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    cf_million_metric_tons_per_TBtu = 138.63/1000000*1000*1E9/2204.62/1000000\n",
    "                #elif fuel == 'wood':\n",
    "                    # No data for wood or biomass here\n",
    "                    # https://www.eia.gov/environment/emissions/co2_vol_mass.php\n",
    "                    # return 0.0\n",
    "                else:\n",
    "                    raise Exception(f'Fuel type {fuel} was not recognized, cannot calculate emissions factor')\n",
    "\n",
    "                # Calculate emssions (million metric tons)\n",
    "                energy_units = row['weighted_energy_consumption_units']\n",
    "                print(energy_units)\n",
    "                assert(energy_units == 'tbtu')\n",
    "                energy_TBtu = row['weighted_energy_consumption']\n",
    "                emis_mmt = energy_TBtu * cf_million_metric_tons_per_TBtu\n",
    "\n",
    "                return emis_mmt\n",
    "\n",
    "            dl['ghg_emissions..million_metric_tons_CO2e)'] = dl.apply(lambda row: emissions(egrid, row), axis=1)\n",
    "\n",
    "            # Drop the in.state_abbreviation column, will be in building characteristics data\n",
    "            dl.drop('in.state', axis=1, inplace=True)\n",
    "        \n",
    "        #Aggegate end uses\n",
    "        end_use_grp = {'ceiling_fan':'miscellaneous', 'clothes_washer':'appliance', 'refrigerator':'appliance', 'plug_loads':'miscellaneous',\n",
    "            'range_oven':'appliance', 'lighting_interior':'lighting', 'mech_vent':'hvac', 'clothes_dryer':'appliance',\n",
    "            'heating_hp_bkup':'hvac', 'cooling':'hvac', 'lighting_garage':'lighting', 'freezer':'appliance',\n",
    "            'heating_fans_pumps':'hvac', 'dishwasher':'appliance', 'cooling_fans_pumps':'hvac',\n",
    "            'heating':'hvac', 'hot_water':'hot water', 'lighting_exterior':'lighting', 'pool_pump':'miscellaneous', 'grill':'miscellaneous',\n",
    "            'pool_heater':'miscellaneous', 'hot_tub_pump':'miscellaneous', 'hot_tub_heater':'miscellaneous', 'well_pump':'miscellaneous',\n",
    "            'lighting':'lighting', 'fireplace':'miscellaneous', 'pv':'miscellaneous'}\n",
    "        \n",
    "        \n",
    "        dl[\"enduse_group\"] = dl[\"enduse\"].map(end_use_grp)\n",
    "        # dl = dl.groupby(['bldg_id','fuel','enduse']).sum().reset_index()\n",
    "        dl = dl.groupby(['bldg_id','fuel','enduse']).sum().reset_index()\n",
    "        dl[\"weighted_energy_consumption_units\"] = dl[\"weighted_energy_consumption_units\"].str[-4:] # Fix units\n",
    "        \n",
    "        # Save files - separate building energy from characteristics for file size\n",
    "        file_name = f'ResStock energy long.csv'\n",
    "        file_path = os.path.join(self.output_dir, file_name)\n",
    "        print(file_path)\n",
    "        dl.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbeb151",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeabe67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating ResStock 2024.1 TMY3\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:98: DtypeWarning: Columns (132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(data_file_path)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:331: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, total_name ] = self.data[emissions_col_list].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:316: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 138.63/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:316: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 138.63/1000000*1000*1E9/2204.62/1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:331: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, total_name ] = self.data[emissions_col_list].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:331: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, total_name ] = self.data[emissions_col_list].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data[new_col] * 141.67/1000000*1000*1E9/2204.62/1000000\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:331: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, total_name ] = self.data[emissions_col_list].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, new_col] = self.data[col] * self.data[self.BLDG_WEIGHT] * conv_fact\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:304: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, emission_col] = self.data.loc[:,new_col] * cf_million_metric_tons_per_TBtu\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, fuel_group_name ] = self.data[x].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:331: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, total_name ] = self.data[emissions_col_list].sum(axis=1)\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:260: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, 'dataset'] = self.dataset_name\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:263: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, 'in.comstock_building_type_group'] = 'Multifamily'\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:197: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, 'in.rentable_floor_area_of_building_this_unit_is_in..ft2'] = self.data['in.geometry_building_number_units_mf'] * self.data[self.FLR_AREA]\n",
      "INFO:__main__:Assuming a rentable area to gross area ratio of 0.75 when setting multifamily building size bins only.\n",
      "INFO:__main__:This is reflected ONLY in \"in.floor_area_category\" and \"in.total_floor_area_of_building_this_unit_is_in..ft2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:It is not reflected in the weighted energy or floor area columns!\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, 'in.total_floor_area_of_building_this_unit_is_in..ft2'] = self.data['in.geometry_building_number_units_mf'] * self.data[self.FLR_AREA] / ber\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:234: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, self.FLR_AREA_CAT] = self.data.apply(lambda row: size_bin(row), axis=1)\n",
      "INFO:__main__:Recalculating weights to represent number of multifamily buildings represented\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:,'units_represented'] = self.data.loc[:,'weight'].copy()\n",
      "/var/folders/0s/fd9ps7m15h735n6l5f6xt06835sn6v/T/ipykernel_24456/2467762815.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data.loc[:, \"bldg_represented\"] = self.data.apply(lambda row: reweight_to_bldg_count(row), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df = ResStock_2(\"baseline_and_annual_results_cluster.csv\") #2 min 20 s\n",
    "# df.export_to_csv_wide()\n",
    "# df.export_to_csv_long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longdf = pd.read_csv('../output/ResStock 2024.1 TMY3/ResStock energy long.csv') #15s\n",
    "# widedf = pd.read_csv('../output/ResStock 2024.1 TMY3/ResStock wide.csv') #1m13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924bb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
