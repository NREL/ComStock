#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import logging
import os

import comstockpostproc as cspp

logging.basicConfig(level='DEBUG')  # Use DEBUG, INFO, or WARNING
logger = logging.getLogger(__name__)

def main():
    # ComStock run
    comstock = cspp.ComStock(
    s3_base_dir='com-sdr',  # If run not on S3, download results_up**.parquet manually
    comstock_run_name='rtuadv_v11',  # Name of the run on S3
    comstock_run_version='rtuadv_v11',  # Use whatever you want to see in plot and folder names
    comstock_year=2018,  # Typically don't change this
    athena_table_name=None,  # Typically don't change this
    truth_data_version='v01',  # Typically don't change this
    buildstock_csv_name='buildstock.csv',  # Download buildstock.csv manually
    acceptable_failure_percentage=0.25,  # Can increase this when testing and high failure are OK
    drop_failed_runs=True,  # False if you want to evaluate which runs failed in raw output data
    color_hex='#0072B2',  # Color used to represent this run in plots
    skip_missing_columns=True,  # False if you want to ensure you have all data specified for export
    reload_from_cache=False, # True if CSV already made and want faster reload times
    include_upgrades=False,  # False if not looking at upgrades
    # output_dir = 's3://oedi-data-lake/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2025/comstock_amy2018_release_1'
    )


    # Stock Estimation for Apportionment:
    stock_estimate = cspp.Apportion(
        stock_estimation_version='2025R3',  # Only updated when a new stock estimate is published
        truth_data_version='v01',  # Typically don't change this
        reload_from_cache=False  # Set to "True" if you have already run apportionment and would like to keep consistant values between postprocessing runs.
    )

    # Scale ComStock runs to the 'truth data' from StockE V3 estimates using bucket-based apportionment
    base_sim_outs = comstock.get_sim_outs_for_upgrade(0)
    comstock.create_allocated_weights(stock_estimate, base_sim_outs, reload_from_cache=False)

    # CBECS
    cbecs = cspp.CBECS(
        cbecs_year=2018,  # 2012 and 2018 currently available
        truth_data_version='v01',  # Typically don't change this
        color_hex='#009E73',  # Color used to represent CBECS in plots
        reload_from_csv=True  # True if CSV already made and want faster reload times
        )

    # First scale ComStock runs to the 'truth data' from StockE V3 estimates using bucket-based apportionment
    # Then scale both ComStock runs to CBECS 2018 AND remove non-ComStock buildings from CBECS
    # This is how weights in the models are set to represent national energy consumption
    base_sim_outs = comstock.get_sim_outs_for_upgrade(0)
    alloc_wts = comstock.get_allocated_weights()
    comstock.create_allocated_weights_scaled_to_cbecs(cbecs, base_sim_outs, alloc_wts, remove_non_comstock_bldg_types_from_cbecs=True)
    comstock.create_allocated_weights_plus_util_bills_for_upgrade(0)


    # county resolution, files by state and county
    county_resolution = {
        'geo_top_dir': 'by_state_and_county',
        'partition_cols': {
            comstock.STATE_ABBRV: 'state',
            comstock.COUNTY_ID: 'county',
        },
        'aggregation_levels': [comstock.COUNTY_ID],
        'data_types': ['full'],
        'file_types': ['parquet'],
    }

    # state level resolution, one single national file
    state_resolution = {
        'geo_top_dir': 'national_by_state',
        'partition_cols': {},
        'aggregation_levels': [[comstock.STATE_ABBRV, comstock.CZ_ASHRAE]],
        'data_types': ['full'], # other options: 'detailed', 'basic' **If using multiple options, order must go from more detailed to less detailed.
        'file_types': ['parquet'], # other options:'parquet'
                        }

    # specify the export level
    # IMPORTANT: if making county level timeseries plots, must export county level data to S3. This does not occur automatically.
    geo_exports = [county_resolution] #state_resolution

    for geo_export in geo_exports:
        # write files locally as needed - usually not needed for AMI comparison plots
        #comstock.export_metadata_and_annual_results_for_upgrade(upgrade_id=0, geo_exports=[geo_export])

        # Also write to S3 if making timeseries plots
        s3_dir = f"s3://{comstock.s3_base_dir}/{comstock.comstock_run_name}/{comstock.comstock_run_name}"
        s3_output_dir = comstock.setup_fsspec_filesystem(s3_dir, aws_profile_name=None)
        comstock.export_metadata_and_annual_results_for_upgrade(upgrade_id=0, geo_exports=[geo_export], output_dir=s3_output_dir)

    # write select results to S3 for Athena/Glue when needed for timeseries plots
    s3_dir = f"s3://{comstock.s3_base_dir}/{comstock.comstock_run_name}/{comstock.comstock_run_name}"
    database = "enduse"
    dataset_name = f"{comstock.comstock_run_name}/{comstock.comstock_run_name}" # name of the dir in s3_dir we want to make new tables and views for
    crawler_name = comstock.comstock_run_name # used to set name of crawler, cannot include slashes
    workgroup = "eulp"  # Athena workgroup to use
    glue_service_role = "service-role/AWSGlueServiceRole-default"

    # Export parquet files to S3 for Athena/Glue
    comstock.create_sightglass_tables(
        s3_location=f"{s3_dir}/metadata_and_annual_results_aggregates",
        dataset_name=crawler_name,
        database_name=database,
        glue_service_role=glue_service_role)
    comstock.fix_timeseries_tables(crawler_name, database)
    comstock.create_views(crawler_name, database, workgroup)

    # AMI
    ami = cspp.AMI(
        truth_data_version='v01',
        reload_from_csv=False
        )
    comstock.download_timeseries_data_for_ami_comparison(ami, reload_from_csv=False, save_individual_regions=False)

    # comparison
    comparison = cspp.ComStockToAMIComparison(comstock, ami, make_comparison_plots=True)
    #comparison.export_plot_data_to_csv_wide()

# Code to execute the script
if __name__ == "__main__":
    main()