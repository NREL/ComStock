{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###import comstockpostproc-standard things, and then don't use most of them\n",
    "\n",
    "import os\n",
    "from textwrap import indent\n",
    "\n",
    "import boto3\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from comstockpostproc.resstock_naming_mixin_LARGEE import ResStockNamingMixin\n",
    "from comstockpostproc.units_mixin import UnitsMixin\n",
    "from comstockpostproc.s3_utilities_mixin import S3UtilitiesMixin\n",
    "from comstockpostproc import resstock_LARGEE\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResStock_data_process():\n",
    "    def __init__(self, resstock_results_folder, resstock_file_name, downselect_rows_tf, downselect_row_fields, \n",
    "                 values_to_keep,  col_plan_folder, col_plan_name,\n",
    "                 add_wide_fields_tf, dfs_for_wide_fields, wide_mergeon_fields, wide_merge_cols, wide_col_plans, wide_merge_newnames,\n",
    "                 add_local_bills_tf, add_first_costs_tf, cost_inputs_folder, cost_inputs_filename, one_year_bill_savings_col, npv_analysis_period, npv_discount_rate,\n",
    "                 downselect_cols_tf, add_long_fields_tf, rate_inputs_df, \n",
    "                 long_fields_also_wide_tf, long_fields_also_wide, long_fields_also_wide_names,\n",
    "                 save_file_tf, output_folder, output_file_name, debug_tf):\n",
    "        \"\"\"\n",
    "        A class to load and transform ResStock 2024.2 data for futher steps in an automated workflow\n",
    "        \"\"\"\n",
    "        #initialize members\n",
    "        self.resstock_results_folder = resstock_results_folder\n",
    "        self.resstock_file_name = resstock_file_name\n",
    "        self.downselect_rows_tf = downselect_rows_tf\n",
    "        self.downselect_row_fields = downselect_row_fields\n",
    "        self.values_to_keep = values_to_keep\n",
    "        self.col_plan_folder = col_plan_folder\n",
    "        self.col_plan_name = col_plan_name\n",
    "        self.add_wide_fields_tf = add_wide_fields_tf\n",
    "        self.dfs_for_wide_fields = dfs_for_wide_fields\n",
    "        self.wide_mergeon_fields = wide_mergeon_fields\n",
    "        self.wide_merge_cols = wide_merge_cols\n",
    "        self.wide_col_plans = wide_col_plans\n",
    "        self.wide_merge_newnames = wide_merge_newnames\n",
    "        self.add_local_bills_tf = add_local_bills_tf\n",
    "        self.add_first_costs_tf = add_first_costs_tf\n",
    "        self.cost_inputs_folder = cost_inputs_folder\n",
    "        self.cost_inputs_filename = cost_inputs_filename\n",
    "        self.one_year_bill_savings_col = one_year_bill_savings_col\n",
    "        self.npv_analysis_period = npv_analysis_period\n",
    "        self.npv_discount_rate = npv_discount_rate\n",
    "        self.downselect_cols_tf = downselect_cols_tf\n",
    "        self.add_long_fields_tf = add_long_fields_tf\n",
    "        self.rate_inputs_df = rate_inputs_df\n",
    "        self.long_fields_also_wide_tf = long_fields_also_wide_tf\n",
    "        self.long_fields_also_wide = long_fields_also_wide\n",
    "        self.long_fields_also_wide_names = long_fields_also_wide_names\n",
    "        self.save_file_tf = save_file_tf\n",
    "        self.output_folder = output_folder\n",
    "        self.output_file_name = output_file_name\n",
    "        self.debug_tf = debug_tf\n",
    "\n",
    "        #execute\n",
    "        self.download_data()\n",
    "        self.downselect_rows()\n",
    "        self.make_col_plan()\n",
    "        self.add_wide_fields()\n",
    "        self.downselect_cols()\n",
    "        self.pivot_data()\n",
    "        self.add_long_fields()\n",
    "        self.categorize_outputs()\n",
    "        self.addl_wide_fields_in_long()\n",
    "        self.add_weighted_values_col()\n",
    "        self.return_and_save_file()\n",
    "\n",
    "    def download_data(self):\n",
    "    #load results from already-downloaded OEDI file\n",
    "        if self.debug_tf == True:\n",
    "            print (1)\n",
    "        results_file_path = os.path.join(self.resstock_results_folder, self.resstock_file_name)\n",
    "        self.data = pd.read_csv(results_file_path, engine = \"pyarrow\")\n",
    "        #self.data = self.data.head(5) #for testing purposes, shrink the data\n",
    "\n",
    "    def downselect_rows(self):\n",
    "    #downselect to a subset of results \n",
    "        if self.debug_tf == True:\n",
    "            print (2)\n",
    "        if(self.downselect_rows_tf == True):\n",
    "            for field, values in zip(self.downselect_row_fields, self.values_to_keep):\n",
    "                self.data = self.data.loc[self.data[field].isin(values)]\n",
    "\n",
    "    def make_col_plan(self):\n",
    "    #assign a plan for each column in the dataset, from a premade csv\n",
    "        if self.debug_tf == True:\n",
    "            print (3)\n",
    "        plan_file_path = os.path.join(self.col_plan_folder, self.col_plan_name)\n",
    "        self.col_plan = pd.read_csv(plan_file_path, engine = \"pyarrow\")\n",
    "        #flag columns in the data that aren't in the column plan\n",
    "        data_cols = self.data.columns.tolist()\n",
    "        cols_in_plan = self.col_plan['column'].tolist()\n",
    "        cols_not_in_plan = list(set(data_cols) - set(cols_in_plan))\n",
    "        if len(cols_not_in_plan) > 0:\n",
    "            print (\"These columns are in the data but not the column plan:\") \n",
    "            print(cols_not_in_plan)\n",
    "        #flag columns in the column plan that will need to be added to the data\n",
    "        cols_not_in_data = list(set(cols_in_plan) - set(data_cols))\n",
    "        if len(cols_not_in_data) > 0:\n",
    "            print (\"These columns are not in the data and will be added, with NaNs:\")\n",
    "            print (cols_not_in_data)\n",
    "        #remake data in standard order and with cols of NAs so that all files have the same columns\n",
    "        for col in cols_not_in_data:\n",
    "            self.data[col] = np.nan\n",
    "        self.data = self.data[cols_in_plan]\n",
    "        #create lists of columns\n",
    "        self.cols_to_remove = self.col_plan.loc[self.col_plan['plan']=='remove', 'column'].tolist()\n",
    "        self.cols_wide = self.col_plan.loc[self.col_plan['plan']=='keep', 'column'].tolist()\n",
    "        self.cols_to_pivot = self.col_plan.loc[self.col_plan['plan']=='pivot', 'column'].tolist()\n",
    "        \n",
    "    def add_wide_fields(self):\n",
    "    #add additional wide format fields before pivoting, and also add plans for them\n",
    "        if self.debug_tf == True:\n",
    "            print (4)\n",
    "        #add additional precomputed wide format fields before pivoting\n",
    "        if(self.add_wide_fields_tf == True):\n",
    "            if self.debug_tf == True:\n",
    "                print (\"4a\")\n",
    "            for dfw, wide_mergeon_field, wide_merge_col, wide_merge_newname, wide_col_plan in zip(\n",
    "                self.dfs_for_wide_fields, self.wide_mergeon_fields, self.wide_merge_cols, self.wide_merge_newnames, self.wide_col_plans):\n",
    "                self.data = self.data.merge(dfw, [[wide_mergeon_field, wide_merge_col]], on = wide_mergeon_field, how = \"left\")\n",
    "                self.data.rename(columns = {wide_merge_col:wide_merge_newname}, inplace = True)\n",
    "                if self.wide_col_plan == 'pivot':\n",
    "                    self.cols_to_pivot = self.cols_to_pivot + [wide_merge_newname]\n",
    "                elif self.wide_col_plan == 'keep':\n",
    "                    self.cols_wide = self.cols_wide + [wide_merge_newname]\n",
    "                else:\n",
    "                    self.cols_to_remove = self.cols_to_remove + [wide_merge_newname]\n",
    "        #add local bills before pivoting\n",
    "        if(self.add_local_bills_tf == True):\n",
    "            if self.debug_tf == True:\n",
    "                print (\"4b\")\n",
    "            for index, row in self.rate_inputs_df.iterrows():\n",
    "                #if all ng consumption is removed, add ng fixed cost to bill savings. So two criteria must be met: there must be natural gas savings and the current natural gas consumption must be 0\n",
    "                if row['column']== \"out.bills_local.natural_gas.total.usd.savings\":\n",
    "                    self.data[row['column']] = row['fixed monthly cost']*12*(np.logical_and((abs(self.data[\"out.natural_gas.total.energy_consumption.kwh.savings\"])>0), \n",
    "                                                                              (self.data[\"out.natural_gas.total.energy_consumption.kwh\"]==0))) + row['variable cost per kwh']*(\n",
    "                                                                            self.data[row['col list for scaling']].sum(axis = 1)) \n",
    "                # for each row of rate inputs, create a column in the data, which will be NaN if the scaling row doesn't exist (e.g savings rows in baseline) and 0 if the relevant consumption rows don't exist\n",
    "                else:\n",
    "                    self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
    "                if row['plan'] == 'pivot':\n",
    "                    self.cols_to_pivot = self.cols_to_pivot + [row['column']]\n",
    "                elif row['plan'] == 'keep':\n",
    "                    self.cols_wide = self.cols_wide + [row['column']]\n",
    "                else:\n",
    "                    self.cols_to_remove = self.cols_to_remove + [row['column']]\n",
    "            plan_for_new_cols_df = self.rate_inputs_df.drop(['fixed monthly cost', 'variable cost per kwh', 'col list for scaling'], axis = 1)\n",
    "            self.col_plan = pd.concat([self.col_plan, plan_for_new_cols_df], axis = 0, ignore_index=True)\n",
    "        #add local first costs (upgrade costs) before pivoting (but don't pivot these, treat them as parameters)\n",
    "        if(self.add_first_costs_tf == True):\n",
    "            #print (\"first costs\")\n",
    "            cost_inputs_filepath = os.path.join(self.cost_inputs_folder, self.cost_inputs_filename)\n",
    "            cost_inputs = pd.read_csv(cost_inputs_filepath, engine = \"pyarrow\")\n",
    "            up_costs = []\n",
    "            for index, row in self.data.iterrows():\n",
    "                cost = 0\n",
    "                upgrade = row[\"upgrade\"]\n",
    "                if upgrade == 0:\n",
    "                    up_costs.append(cost)\n",
    "                elif (row['applicability']!=True):\n",
    "                    up_costs.append(cost)\n",
    "                else:     #actually calculate costs          \n",
    "                    #extract necessary data\n",
    "                    location = row[cost_inputs['Location Field Match'][0]] #just using whatever geographic resolution the first row of Cost Inputs has, for now\n",
    "                    climate_zone = int(row[\"in.ashrae_iecc_climate_zone_2004\"][0]) #just the number, not the letter\n",
    "                    hp_size_btuh = row[\"out.params.size_heating_system_primary_k_btu_h\"]\n",
    "                    hp_size_tons = (hp_size_btuh *1000)/12000\n",
    "                    attic_floor_area_sf = row[\"out.params.floor_area_attic_ft_2\"]\n",
    "                    num_exterior_doors = row[\"out.params.door_area_ft_2\"]/20 #ResStock 2024.2 has 20ft2 total door area for any unit with exterior doors, which is approximately one door\n",
    "                    num_windows = row[\"out.params.window_area_ft_2\"]/15 #15 ft2 seems like a decent proxy for average window size based on standard window sizes\n",
    "                    HPWH_gal = row[\"out.params.size_water_heater_gal\"]\n",
    "                    pool_heater_tons = 1 #proxy for all pool heaters, based loosely on looking at availability at Home Depot website\n",
    "                    spa_heater_tons = 1 #proxy for all spa heaters\n",
    "                    applicability_criteria_1 = row[cost_inputs['Applicability Criteria Field1'][0]]#for now this only works if there's just one applicability critiera field that's constant for the whole cost inputs dataset\n",
    "\n",
    "                    # look up sum spec name on upgrade, location, and applicability criteria\n",
    "                    if row['upgrade'] ==16: #there's no applicability critieria for this one\n",
    "                        selected_index = cost_inputs[np.logical_and(cost_inputs['Location Value Match'] == location, \n",
    "                                                cost_inputs[\"Upgrade\"] == upgrade)].index.values.astype(int)[0]\n",
    "                    else:\n",
    "                        selected_index = cost_inputs[np.logical_and(np.logical_and(cost_inputs['Location Value Match'] == location, \n",
    "                                                cost_inputs[\"Upgrade\"] == upgrade),\n",
    "                                                cost_inputs['Applicability Criteria Values1'] == applicability_criteria_1)].index.values.astype(int)[0]\n",
    "                        #print(selected_index)\n",
    "                \n",
    "                    #extract all the right values for the sum spec name (this is cols H through X of the \"Cost Inputs.csv\" as of 2025-02-13)\n",
    "                    hp_cost_per_ton = cost_inputs.loc[selected_index, \"HP Cost Per Ton\"]\n",
    "                    hpwh_cost_per_gal = cost_inputs.loc[selected_index, \"HPWH Cost Per Gallon\"]\n",
    "                    pool_heater_cost_per_ton = cost_inputs.loc[selected_index, \"Pool Heater Cost Per Ton\"]\n",
    "                    spa_heater_cost_per_ton = cost_inputs.loc[selected_index, \"Spa Heater Cost Per Ton\"]\n",
    "                    calc1_constant = cost_inputs.loc[selected_index, \"Calc1 Constant\"]\n",
    "                    calc2_constant1 = cost_inputs.loc[selected_index, \"Calc2 Constant1\"]\n",
    "                    calc2_constant2 = cost_inputs.loc[selected_index, \"Calc2 Constant2\"]\n",
    "                    calc3_constant1 = cost_inputs.loc[selected_index, \"Calc3 Constant1\"]\n",
    "                    calc3_constant2 = cost_inputs.loc[selected_index, \"Calc3 Constant2\"]\n",
    "                    calc4_constant1 = cost_inputs.loc[selected_index, \"Calc4 Constant1\"]\n",
    "                    calc4_constant2 = cost_inputs.loc[selected_index, \"Calc4 Constant2\"]\n",
    "                    calc1_coeff = cost_inputs.loc[selected_index, \"Calc1 Coeff\"]\n",
    "                    calc2_coeff = cost_inputs.loc[selected_index, \"Calc2 Coeff\"]\n",
    "                    calc3_coeff = cost_inputs.loc[selected_index, \"Calc3 Coeff\"]\n",
    "                    calc4_coeff = cost_inputs.loc[selected_index, \"Calc4 Coeff\"]\n",
    "                    fixed_costs_demo = cost_inputs.loc[selected_index, \"Fixed Costs Demo\"]\n",
    "                    fixed_costs_install = cost_inputs.loc[selected_index, \"Fixed Costs Install\"]\n",
    "                    #print(hp_cost_per_ton, hpwh_cost_per_gal, pool_heater_cost_per_ton, spa_heater_cost_per_ton, calc1_constant, calc2_constant1,\n",
    "                    #      calc2_constant2, calc3_constant1, calc3_constant2, calc4_constant1, calc4_constant2, calc1_coeff, calc2_coeff, calc3_coeff, \n",
    "                    #      calc4_coeff, fixed_costs_demo, fixed_costs_install)\n",
    "                    #do the algabraic cost calc\n",
    "                    calc1 = attic_floor_area_sf * calc1_constant * calc1_coeff\n",
    "                    calc2 = (num_exterior_doors * calc2_constant1 + num_windows * calc2_constant2) * calc2_coeff\n",
    "                    if climate_zone < 4:\n",
    "                        calc3 = calc3_constant1 * attic_floor_area_sf\n",
    "                    else:\n",
    "                        calc3 = calc3_constant2 * attic_floor_area_sf\n",
    "                    if (climate_zone > 1 and climate_zone) < 4:\n",
    "                        calc4 = calc4_constant1 * attic_floor_area_sf\n",
    "                    else:\n",
    "                        calc4 = calc4_constant2 * attic_floor_area_sf\n",
    "                    cost = cost + (hp_size_tons * hp_cost_per_ton) + ( #coeffs are 0 where not applicable\n",
    "                        HPWH_gal * hpwh_cost_per_gal) + (\n",
    "                            pool_heater_tons * pool_heater_cost_per_ton) + (\n",
    "                                spa_heater_tons * spa_heater_cost_per_ton) + (\n",
    "                                    calc1 * calc1_coeff) + (\n",
    "                                        calc2 * calc2_coeff) + (\n",
    "                                            calc3 * calc3_coeff) + (\n",
    "                                                calc4 * calc4_coeff) + (\n",
    "                                                    fixed_costs_demo + fixed_costs_install)\n",
    "                    up_costs.append(float(cost))\n",
    "                #print (upgrade)\n",
    "                #print (cost)\n",
    "            #assign new column of first costs\n",
    "            self.data['out.first_costs.usd'] = up_costs\n",
    "            #assign new column of simple payback periods. Note this is currently not robust at all, assumes there is a column called \"out.bills_local.all_fuels.total.usd.savings\" which there won't always be (e.g. if you are using direct ResStock bill calcs)\n",
    "            self.data['out.simple_payback_period'] = self.data['out.first_costs.usd']/self.data[self.one_year_bill_savings_col]\n",
    "            #calculate NPV\n",
    "            npv_list = []\n",
    "            for id_cost, id_savings in zip(self.data['out.first_costs.usd'], self.data[self.one_year_bill_savings_col]):\n",
    "                cost_array = [0]*(self.npv_analysis_period + 1)\n",
    "                cost_array[0] = id_cost\n",
    "                savings_array = id_savings * (self.npv_analysis_period + 1)\n",
    "                cash_flows = list(np.array(savings_array)-np.array(cost_array))\n",
    "                npv = 0\n",
    "                for year in range(0, self.npv_analysis_period + 1):\n",
    "                    npv += (1/((1+self.npv_discount_rate)**year)) * cash_flows[year]\n",
    "                npv_list.append(npv)\n",
    "            self.data['out.net_present_value'] = npv_list\n",
    "            #add new cost-related to columns to the list of columns to be kept wide (not pivoted)\n",
    "            self.cols_wide = self.cols_wide + ['out.first_costs.usd', 'out.simple_payback_period', 'out.net_present_value']\n",
    "            #add new cost-related columns to the column plan\n",
    "            first_costs_col_plan = pd.DataFrame({'column': ['out.first_costs.usd', 'out.simple_payback_period', 'out.net_present_value'], \n",
    "                                                 'col_type': ['unique', 'unique', 'unique'], 'plan': ['keep', 'keep', 'keep'], \n",
    "                                    'Result Type': ['First Cost', 'Simple Payback Period', 'Net Present Value'],\n",
    "                                     'Fuel': ['NA', 'NA', 'NA'], 'End Use': ['NA', 'NA', 'NA'], 'End Use Category':['NA', 'NA', 'NA']})\n",
    "            self.col_plan = pd.concat([self.col_plan, first_costs_col_plan], axis = 0, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    def downselect_cols(self):\n",
    "    #remove unneceessary columns\n",
    "        if self.debug_tf == True:\n",
    "            print (5)\n",
    "        if(self.downselect_cols_tf == True):\n",
    "            self.data.drop(self.data[self.cols_to_remove], axis = 1, inplace = True)\n",
    "            #print(self.data.columns)\n",
    "\n",
    "    def pivot_data(self):\n",
    "    #make all the results long format, keep the characteristics wide\n",
    "        if self.debug_tf == True:\n",
    "            print (6)\n",
    "        self.data_long = pd.melt(\n",
    "            self.data,\n",
    "            id_vars = self.cols_wide,\n",
    "            var_name = \"Output\",\n",
    "            value_name = \"Value\"\n",
    "        )\n",
    "    \n",
    "    def add_long_fields(self):\n",
    "    #this is where you add any long format fields.\n",
    "        if self.debug_tf == True:\n",
    "            print (7)\n",
    "        if(self.add_long_fields_tf == True):\n",
    "            if self.debug_tf == True:\n",
    "                print (1)\n",
    "\n",
    "    def categorize_outputs(self):\n",
    "    #Develop output categorization\n",
    "        if self.debug_tf == True:\n",
    "            print (8)\n",
    "        #use mappings to get the categorizations\n",
    "        out_cats = self.col_plan.drop(self.col_plan[[\"col_type\", \"plan\"]], axis = 1, inplace = False)\n",
    "        self.data_long = self.data_long.merge(out_cats, left_on = 'Output', right_on = \"column\", how = 'left')\n",
    "\n",
    "    def addl_wide_fields_in_long(self):\n",
    "    #re-merge in any long fields that are also needed as wide fields\n",
    "        if self.debug_tf == True:\n",
    "            print (9)\n",
    "        if(self.long_fields_also_wide_tf == True):\n",
    "            merge_data_cols = [\"bldg_id\"] + self.long_fields_also_wide\n",
    "            self.data_long = self.data_long.merge(self.data[merge_data_cols], on = \"bldg_id\", how = \"left\")\n",
    "            for colname, newcolname in zip(self.long_fields_also_wide, self.long_fields_also_wide_names):\n",
    "                self.data_long.rename(columns = {colname:newcolname}, inplace = True)\n",
    "\n",
    "    def add_weighted_values_col(self):\n",
    "    #add a column with the weighted value alongside the unweighted value column\n",
    "        if self.debug_tf == True:\n",
    "            print (10)\n",
    "        self.data_long['Weighted Value'] = self.data_long['Value']*self.data_long['weight']\n",
    "\n",
    "    def return_and_save_file(self):\n",
    "    #save file\n",
    "        if self.debug_tf == True:\n",
    "            print (11)\n",
    "        return self.data_long\n",
    "        if self.save_file_tf == True:\n",
    "            self.data_long.to_csv(os.path.join(self.output_folder, self.output_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0491440437526616\n",
      "0.06998572515142275\n",
      "0.11916420397790706\n"
     ]
    }
   ],
   "source": [
    "####Prepare utility rates for C2C DV\n",
    "\n",
    "##project-specific utility bills - inputs\n",
    "#Electricity\n",
    "fixed_elec_cost_monthly = 10.56\n",
    "var_elec_cost_per_kwh = 0.17404 #cf 0.137/kwh\n",
    "\n",
    "#Natural Gas\n",
    "fixed_ng_cost_monthly = 16.25\n",
    "var_ng_cost_per_ccf = 1.495\n",
    "\n",
    "#Fuel Oil\n",
    "var_fo_cost_per_gal = 2.851\n",
    "\n",
    "#Propane\n",
    "var_propane_cost_per_gal = 3.199\n",
    "\n",
    "##project-specific utility bills - unit conversions\n",
    "gal_fuel_oil_to_mbtu = 139/1000\n",
    "gal_propane_to_mbtu = 91.6 / 1000\n",
    "mbtu_to_kwh = 293.0710701722222\n",
    "dol_per_ccf_to_dol_per_therm = 1/1.038 #$ per Ccf divided by 1.038 equals $ per therm https://www.eia.gov/tools/faqs/faq.php?id=45&t=8\n",
    "therm_to_kwh = 29.307107017222222\n",
    "\n",
    "var_ng_cost_per_kwh = var_ng_cost_per_ccf * (dol_per_ccf_to_dol_per_therm) * (1/therm_to_kwh)\n",
    "var_fo_cost_per_kwh = var_fo_cost_per_gal * (1/gal_fuel_oil_to_mbtu) * (1/mbtu_to_kwh)\n",
    "var_propane_cost_per_kwh = var_propane_cost_per_gal * (1/gal_propane_to_mbtu) * (1/mbtu_to_kwh)\n",
    "\n",
    "print(var_ng_cost_per_kwh) #0.0491440437526616, cf 0.0339307/kwh\n",
    "print(var_fo_cost_per_kwh) #0.06998572515142275, cf 0.0704125/kwh\n",
    "print(var_propane_cost_per_kwh) #0.11916420397790706. cf 0.101456/kWh\n",
    "\n",
    "#assemble for input\n",
    "rates_data_inputs = [\n",
    "    [\"out.bills_local.electricity.total.usd\", fixed_elec_cost_monthly, var_elec_cost_per_kwh, [\"out.electricity.total.energy_consumption.kwh\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bills\", \"Electricity\", \"Electricity Total\", \"Total\"],\n",
    "    [\"out.bills_local.natural_gas.total.usd\", fixed_ng_cost_monthly, var_ng_cost_per_kwh, [\"out.natural_gas.total.energy_consumption.kwh\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bills\", \"Natural Gas\", \"Natural Gas Total\", \"Total\"],\n",
    "    [\"out.bills_local.fuel_oil.total.usd\", 0, var_fo_cost_per_kwh, [\"out.fuel_oil.total.energy_consumption.kwh\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bills\", \"Fuel Oil\", \"Fuel Oil Total\", \"Total\"],\n",
    "    [\"out.bills_local.propane.total.usd\", 0, var_propane_cost_per_kwh, [\"out.propane.total.energy_consumption.kwh\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bills\", \"Propane\", \"Total\", \"Total\"],\n",
    "    [\"out.bills_local.all_fuels.total.usd\", 0, 1, [\"out.bills_local.electricity.total.usd\", \"out.bills_local.natural_gas.total.usd\", \"out.bills_local.fuel_oil.total.usd\", \"out.bills_local.propane.total.usd\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bill Totals\", \"Energy\", \"Total\", \"Total\"],\n",
    "    [\"out.bills_local.electricity.total.usd.savings\", 0, var_elec_cost_per_kwh, [\"out.electricity.total.energy_consumption.kwh.savings\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bills\", \"Electricity\", \"Electricity Total\", \"Total\"],\n",
    "    [\"out.bills_local.natural_gas.total.usd.savings\", fixed_ng_cost_monthly, var_ng_cost_per_kwh, [\"out.natural_gas.total.energy_consumption.kwh.savings\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bill Savings\", \"Natural Gas\", \"Natural Gas Total\", \"Total\"],\n",
    "    [\"out.bills_local.fuel_oil.total.usd.savings\", 0, var_fo_cost_per_kwh, [\"out.fuel_oil.total.energy_consumption.kwh.savings\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bill Savings\", \"Fuel Oil\", \"Fuel Oil Total\", \"Total\"],\n",
    "    [\"out.bills_local.propane.total.usd.savings\", 0, var_propane_cost_per_kwh, [\"out.propane.total.energy_consumption.kwh.savings\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bill Savings\", \"Propane\", \"Propane Total\", \"Total\"],\n",
    "    [\"out.bills_local.all_fuels.total.usd.savings\", 0, 1, [\"out.bills_local.electricity.total.usd.savings\", \"out.bills_local.natural_gas.total.usd.savings\", \"out.bills_local.fuel_oil.total.usd.savings\", \"out.bills_local.propane.total.usd.savings\"], \n",
    "     \"out.x\", \"pivot\", \"Utility Bills Savings Totals\", \"Energy\", \"Total\", \"Total\"]\n",
    "]\n",
    "\n",
    "one_year_bill_savings_col = \"out.bills_local.all_fuels.total.usd.savings\"\n",
    "rate_inputs_df = pd.DataFrame(rates_data_inputs, columns = ['column', 'fixed monthly cost', 'variable cost per kwh', 'col list for scaling', 'col_type', 'plan', 'Result Type', 'Fuel', 'End Use', 'End Use Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['out.emissions_reduction.electricity.lrmer_high_re_cost_15.co2e_kg', 'out.electricity.cooling.energy_consumption.kwh.savings', 'out.natural_gas.pool_heater.energy_consumption.kwh.savings', 'upgrade.heating_setpoint_offset_magnitude', 'upgrade.hvac_secondary_heating_efficiency', 'out.electricity.pool_pump.energy_consumption.kwh.savings', 'out.emissions_reduction.propane.lrmer_mid_case_25.co2e_kg', 'upgrade.clothes_dryer', 'upgrade.cooling_setpoint_has_offset', 'out.hot_water.dishwasher.gal.savings', 'out.bills.all_fuels.usd.savings', 'out.load.heating.peak.kbtu_hr.savings', 'upgrade.cooking_range', 'out.hot_water.fixtures.gal.savings', 'out.emissions_reduction.propane.lrmer_mid_case_15.co2e_kg', 'upgrade.misc_hot_tub_spa', 'out.emissions_reduction.all_fuels.lrmer_mid_case_15.co2e_kg', 'out.hot_water.distribution_waste.gal.savings', 'out.natural_gas.range_oven.energy_consumption.kwh.savings', 'out.propane.heating.energy_consumption.kwh.savings', 'out.emissions_reduction.fuel_oil.lrmer_high_re_cost_15.co2e_kg', 'out.emissions_reduction.electricity.lrmer_low_re_cost_15.co2e_kg', 'out.emissions_reduction.all_fuels.lrmer_high_re_cost_15.co2e_kg', 'out.electricity.clothes_dryer.energy_consumption.kwh.savings', 'out.natural_gas.grill.energy_consumption.kwh.savings', 'out.electricity.well_pump.energy_consumption.kwh.savings', 'out.electricity.dishwasher.energy_consumption.kwh.savings', 'out.electricity.heating_hp_bkup_fa.energy_consumption.kwh.savings', 'out.electricity.range_oven.energy_consumption.kwh.savings', 'out.electricity.summer.peak.kw.savings', 'upgrade.water_heater_efficiency', 'out.energy_burden.percentage.savings', 'out.emissions_reduction.fuel_oil.lrmer_mid_case_25.co2e_kg', 'out.electricity.heating_hp_bkup.energy_consumption.kwh.savings', 'upgrade.hvac_secondary_heating_fuel', 'upgrade.infiltration_reduction', 'out.emissions_reduction.electricity.lrmer_mid_case_25.co2e_kg', 'out.bills.fuel_oil.usd.savings', 'out.propane.total.energy_consumption.kwh.savings', 'out.propane.hot_water.energy_consumption.kwh.savings', 'out.natural_gas.fireplace.energy_consumption.kwh.savings', 'upgrade.hvac_heating_efficiency', 'out.emissions_reduction.natural_gas.lrmer_mid_case_15.co2e_kg', 'upgrade.hvac_cooling_partial_space_conditioning', 'out.electricity.heating.energy_consumption.kwh.savings', 'out.emissions_reduction.natural_gas.lrmer_high_re_cost_15.co2e_kg', 'out.electricity.lighting_garage.energy_consumption.kwh.savings', 'out.emissions_reduction.fuel_oil.lrmer_low_re_cost_15.co2e_kg', 'out.hot_water.clothes_washer.gal.savings', 'out.unmet_hours.cooling.hour.savings', 'upgrade.misc_pool_heater', 'out.load.cooling.peak.kbtu_hr.savings', 'out.electricity.net.energy_consumption.kwh.savings', 'out.site_energy.total.energy_consumption.kwh.savings', 'out.emissions_reduction.fuel_oil.lrmer_mid_case_15.co2e_kg', 'upgrade_name', 'out.electricity.mech_vent.energy_consumption.kwh.savings', 'out.electricity.hot_water.energy_consumption.kwh.savings', 'out.electricity.pv.energy_consumption.kwh.savings', 'out.electricity.lighting_exterior.energy_consumption.kwh.savings', 'out.natural_gas.permanent_spa_heat.energy_consumption.kwh.savings', 'out.natural_gas.heating.energy_consumption.kwh.savings', 'out.natural_gas.lighting.energy_consumption.kwh.savings', 'out.emissions_reduction.propane.lrmer_high_re_cost_15.co2e_kg', 'out.natural_gas.heating_hp_bkup.energy_consumption.kwh.savings', 'out.propane.heating_hp_bkup.energy_consumption.kwh.savings', 'upgrade.hvac_cooling_efficiency', 'upgrade.heating_setpoint_has_offset', 'upgrade.water_heater_fuel', 'out.bills.electricity.usd.savings', 'out.bills.natural_gas.usd.savings', 'out.emissions_reduction.natural_gas.lrmer_mid_case_25.co2e_kg', 'out.propane.range_oven.energy_consumption.kwh.savings', 'out.electricity.permanent_spa_pump.energy_consumption.kwh.savings', 'out.electricity.total.energy_consumption.kwh.savings', 'out.fuel_oil.total.energy_consumption.kwh.savings', 'out.fuel_oil.hot_water.energy_consumption.kwh.savings', 'upgrade.insulation_ceiling', 'out.load.cooling.energy_delivered.kbtu.savings', 'out.emissions_reduction.propane.lrmer_low_re_cost_15.co2e_kg', 'out.electricity.lighting_interior.energy_consumption.kwh.savings', 'out.unmet_hours.heating.hour.savings', 'out.electricity.freezer.energy_consumption.kwh.savings', 'out.electricity.cooling_fans_pumps.energy_consumption.kwh.savings', 'upgrade.heating_setpoint_offset_period', 'out.electricity.winter.peak.kw.savings', 'upgrade.cooling_setpoint_offset_magnitude', 'out.bills.propane.usd.savings', 'out.propane.clothes_dryer.energy_consumption.kwh.savings', 'out.electricity.ceiling_fan.energy_consumption.kwh.savings', 'out.natural_gas.hot_water.energy_consumption.kwh.savings', 'out.emissions_reduction.all_fuels.lrmer_mid_case_25.co2e_kg', 'out.emissions_reduction.all_fuels.lrmer_low_re_cost_15.co2e_kg', 'out.fuel_oil.heating.energy_consumption.kwh.savings', 'out.electricity.plug_loads.energy_consumption.kwh.savings', 'out.electricity.pool_heater.energy_consumption.kwh.savings', 'out.electricity.clothes_washer.energy_consumption.kwh.savings', 'out.natural_gas.clothes_dryer.energy_consumption.kwh.savings', 'out.emissions_reduction.electricity.lrmer_mid_case_15.co2e_kg', 'upgrade.cooling_setpoint_offset_period', 'out.natural_gas.total.energy_consumption.kwh.savings', 'out.electricity.refrigerator.energy_consumption.kwh.savings', 'out.emissions_reduction.natural_gas.lrmer_low_re_cost_15.co2e_kg', 'out.load.heating.energy_delivered.kbtu.savings', 'out.load.hot_water.energy_delivered.kbtu.savings', 'out.electricity.heating_fans_pumps.energy_consumption.kwh.savings', 'out.electricity.permanent_spa_heat.energy_consumption.kwh.savings', 'out.fuel_oil.heating_hp_bkup.energy_consumption.kwh.savings', 'out.site_energy.net.energy_consumption.kwh.savings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[col] = np.nan\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*(np.logical_and((abs(self.data[\"out.natural_gas.total.energy_consumption.kwh.savings\"])>0),\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[row['column']] = row['fixed monthly cost']*12*((self.data[row['col list for scaling']].sum(axis = 1))!=0) + row['variable cost per kwh']*(self.data[row['col list for scaling']].sum(axis = 1))\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data['out.first_costs.usd'] = up_costs\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data['out.simple_payback_period'] = self.data['out.first_costs.usd']/self.data[self.one_year_bill_savings_col]\n",
      "C:\\Users\\epresent\\AppData\\Local\\Temp\\1\\ipykernel_21672\\194835276.py:235: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data['out.net_present_value'] = npv_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upgrade02\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.insulation_ceiling', 'upgrade.hvac_secondary_heating_fuel', 'upgrade.hvac_secondary_heating_efficiency', 'upgrade.infiltration_reduction', 'upgrade.cooking_range', 'upgrade.water_heater_fuel', 'upgrade.water_heater_efficiency', 'upgrade.misc_hot_tub_spa', 'upgrade.clothes_dryer', 'upgrade.misc_pool_heater']\n",
      "upgrade04\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.insulation_ceiling', 'upgrade.infiltration_reduction', 'upgrade.cooking_range', 'upgrade.water_heater_fuel', 'upgrade.water_heater_efficiency', 'upgrade.misc_hot_tub_spa', 'upgrade.clothes_dryer', 'upgrade.misc_pool_heater']\n",
      "upgrade07\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.hvac_secondary_heating_fuel', 'upgrade.hvac_secondary_heating_efficiency', 'upgrade.cooking_range', 'upgrade.water_heater_fuel', 'upgrade.water_heater_efficiency', 'upgrade.misc_hot_tub_spa', 'upgrade.clothes_dryer', 'upgrade.misc_pool_heater']\n",
      "upgrade09\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.cooking_range', 'upgrade.water_heater_fuel', 'upgrade.water_heater_efficiency', 'upgrade.misc_hot_tub_spa', 'upgrade.clothes_dryer', 'upgrade.misc_pool_heater']\n",
      "upgrade12\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.hvac_secondary_heating_fuel', 'upgrade.hvac_secondary_heating_efficiency']\n",
      "upgrade13\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.hvac_secondary_heating_fuel', 'upgrade.hvac_secondary_heating_efficiency']\n",
      "upgrade16\n",
      "These columns are not in the data and will be added, with NaNs:\n",
      "['upgrade.cooling_setpoint_offset_period', 'upgrade.heating_setpoint_offset_magnitude', 'upgrade.hvac_secondary_heating_fuel', 'upgrade.hvac_secondary_heating_efficiency', 'upgrade.heating_setpoint_offset_period', 'upgrade.hvac_cooling_partial_space_conditioning', 'upgrade.cooling_setpoint_has_offset', 'upgrade.hvac_cooling_efficiency', 'upgrade.cooking_range', 'upgrade.heating_setpoint_has_offset', 'upgrade.water_heater_fuel', 'upgrade.water_heater_efficiency', 'upgrade.misc_hot_tub_spa', 'upgrade.clothes_dryer', 'upgrade.hvac_heating_efficiency', 'upgrade.misc_pool_heater', 'upgrade.cooling_setpoint_offset_magnitude']\n"
     ]
    }
   ],
   "source": [
    "# process multiple sets of data\n",
    "up_list = [\"baseline\", \"upgrade02\", \"upgrade04\", \"upgrade07\", \"upgrade09\", \"upgrade12\", \"upgrade13\", \"upgrade16\"]\n",
    "processed_data = []\n",
    "for up in up_list:\n",
    "    print (up)\n",
    "    resstock_file_name = \"PA_\" + up + \"_metadata_and_annual_results.csv\"\n",
    "    outname = \"PA_\" + up + \"processed_results.csv\"\n",
    "    results = ResStock_data_process(\n",
    "        resstock_results_folder = \"C:/Users/epresent/NREL/BuildStock Analysis User Engagement-C2C Delaware - Documents/10_Analysis/Data/2024.2/AMY2018\",\n",
    "        resstock_file_name = resstock_file_name,\n",
    "        downselect_rows_tf = True, \n",
    "        downselect_row_fields = [\"in.county_name\"],\n",
    "        values_to_keep = [[\"Montgomery County\", \"Bucks County\", \"Chester County\", \"Delaware County\"]],\n",
    "        col_plan_folder = 'C:/Users/epresent/NREL/BuildStock Analysis User Engagement-C2C Delaware - Documents/10_Analysis/Upgrade (4.3)',\n",
    "        col_plan_name = '2024-2 Col Plan including C2C DV Upgrades.csv',\n",
    "        add_wide_fields_tf = False,\n",
    "        dfs_for_wide_fields = 'NA',\n",
    "        wide_mergeon_fields = 'NA',\n",
    "        wide_merge_cols = 'NA', \n",
    "        wide_col_plans = 'NA',\n",
    "        wide_merge_newnames = 'NA',\n",
    "        add_local_bills_tf = True,\n",
    "        add_first_costs_tf = True, \n",
    "        cost_inputs_folder = 'C:/Users/epresent/NREL/BuildStock Analysis User Engagement-C2C Delaware - Documents/10_Analysis/Cost Data', \n",
    "        cost_inputs_filename = 'Cost Inputs.csv',\n",
    "        one_year_bill_savings_col = one_year_bill_savings_col,\n",
    "        npv_analysis_period = 15,\n",
    "        npv_discount_rate = 0.05,\n",
    "        downselect_cols_tf = True, \n",
    "        add_long_fields_tf = False, \n",
    "        rate_inputs_df = rate_inputs_df, \n",
    "        long_fields_also_wide_tf = True,\n",
    "        long_fields_also_wide = [\"out.emissions.all_fuels.lrmer_mid_case_15.co2e_kg\", \"out.bills_local.all_fuels.total.usd\"], \n",
    "        long_fields_also_wide_names = [\"Emissions\", \"Utility Bills Total\"],\n",
    "        save_file_tf = True,\n",
    "        output_folder = \"C:/Users/epresent/NREL/BuildStock Analysis User Engagement-C2C Delaware - Documents/10_Analysis/Upgrade (4.3)\", \n",
    "        output_file_name = outname, \n",
    "        debug_tf = False\n",
    "        )\n",
    "    processed_data = processed_data + [results.data_long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "7\n",
      "9\n",
      "12\n",
      "13\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#save multiple sets of processed data \n",
    "\n",
    "##set destination\n",
    "output_folder = \"C:/Users/epresent/NREL/BuildStock Analysis User Engagement-C2C Delaware - Documents/10_Analysis/Upgrade (4.3)\"\n",
    "output_file_name = \"C2C_DV_data_with_ups.csv\"\n",
    "output_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "#save first set of data, with headers\n",
    "processed_data[0].to_csv(output_path, header = True, index = False)\n",
    "\n",
    "#save remaining data, appending it to the same file. This should be fine since all the datasets should now be generating the same set of columns in the same order.\n",
    "for dataset in processed_data[1:]:\n",
    "    print(dataset[\"upgrade\"][0])\n",
    "    dataset.to_csv(output_path, header = False, index = False, mode = 'a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comstockpostproc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
