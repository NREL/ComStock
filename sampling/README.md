# ComStock Sampling
This documentation outlines how to generate a sample to be run in ComStock.
Before running these scripts, please set up credentials for accessing the S3 RESBLDG account and create a new conda environment.

## Installation
Navigate to the `/sampling` directory of this repo:
```
$ cd /path/to/comstock/sampling
```

Create a new environment from the environment.yml file containing the packages needed for this repository:
```
$ conda env create -f environment.yml
```

This only needs to be done once because it will create a new environment called `comstock-sampling`. Before you begin running the sampling scripts, activate the environment:
```
$ conda activate comstock-sampling
```

Install buildstockbatch in comstock-sampling environment: navigate to location of buildstock batch, e.g.:
```
$ cd /path/to/buildstockbatch
$ python -m pip install -e .
```

## AWS Access

### Non-NREL Staff

To download your BuildStockBatch simulation results from S3 for postprocessing, you’ll need to configure your user account with your AWS credentials. This setup only needs to be done once.

1. [Install the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) version 2
2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration). (Don’t type the `$` in the example.)
3. You may need to [change the Athena Engine version](https://docs.aws.amazon.com/athena/latest/ug/engine-versions-changing.html) for your query workgroup to v2 or v3.

### NREL Staff

NREL now uses a refreshable Single Sign On (SSO) approach for authentication of accounts.

### If you have already configured the SSO for the resbldg AWS account:

1. Set the `AWS_DEFAULT_PROFILE` environment variable to the alias for your AWS resbldg SSO account.
    - On Windows, the command is `set AWS_DEFAULT_PROFILE=my_resbldg_account_alias`
    - On OSX, the command is `export AWS_DEFAULT_PROFILE=my_resbldg_account_alias`
2. Run the following command to activate the SSO: `aws sso login` - follow the prompts in the webpage

You're now ready to execute the commands below! In case of an AWS access error please begin by running the login command again. The SSO does time out eventually.

### To configure SSO for the resbldg account

Note: Access to the SSO requires an NREL network account. We do not currently support use of the sampler for users without an NREL account.

1. Go to the [NREL AWS SSO page](https://nrel-ace.awsapps.com/start#/) and click on the AWS Account button.
2. Click on the NREL AWS RESBLDG dropdown. If you do not see the dropdown email the [CSC team](mailto:StratusCloudHelp@nrel.gov) and ask for accesss to the resbldg account.
3. Click on an available role (typically `developer`) and then click the `Command line or programatic access` link.
4. Follow the steps listed in the `AWS IAM Identity Center credentials (Recommended)` section.
5. Remember the name you give the profile during the configuration. This is the value you will set the `AWS_DEFAULT_PROFILE` enviornment variable to.
6. Open the `credentials` file inside your home directory:
    - On Windows, this is: `C:\Users\myusername\.aws\credentials`
    - On Mac, this is: `/Users/myusername/.aws/credentials`
7. If there are any values set under the `default` profile either rename the profile (replace the word `default` with something else) or delete the section. For reference a default profile in the `credentials` file looks like the following:
    ```
    [default]
    aws_access_key_id = AKIAIOSFODNN7EXAMPLE
    aws_secret_access_key = wJalrX+UtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
    ```
8. Follow the steps above to set the `AWS_DEFAULT_PROFILE` and login to the resbldg account.


## Generating a Sample
There are two steps to generating a sample to be run using ComStock:
1. Generate a `buildstock.csv` file with the main building characteristics.
2. Join geospatial columns to the `buildstock.csv`.

### Generating a `buildstock.csv` for ComStock
There are two steps for generating a ComStock sample file. First, the `tsv_sampling.py` file is used to generate a `buildstock.csv` file that reflects the TSVs used, and second the `join_geospatial.py` file is used to add relevant geographic columns to buildstock.csv generated by `tsv_sampling.py`.

#### Generating a sample of the TSVs using `tsv_sampling.py`
National sample using `tsv_sampling.py` now relies on an input CSV file called the bucket definition file. Functionally, this file looks like a buildstock.csv file but only with the columns `Building`, `sampling_region`, `building_type`, `size_bin`, `heating_fuel`, and `hvac_system_type`. This file has one row for each sample, just like the `buildstock.csv` file, however this file is generated using the `comstock_allocation.py` file in the ComStockPostProcessing package. This file specifies what combinations of the previously listed attributes will be sampled (based on the combination of column values) and how often (the number of rows with a specific combination determines how many samples will be drawn.)

Currently two bucket definition files are available. Both were generated for EUSS 2024 R2. They are a test file, named `euss24r2_sample_initialization_test.csv`, and a production file named `euss24r2_sample_initialization_test.csv`. The only difference betwen these two files is the number of building energy model samples per bucket, with test hving one per builidng energy model sample and productions runs 12 per combination of primary variables.

To start the TSV generation process do the following:

- From this directory, run the following command:

```
$ python tsv_sampling.py tsv_version sim_year n_samples n_buckets hvac_sizing -p bucket_definition_file.csv
```

- Where:
    - `tsv_version` is the version of tsv files to sample from (e.g., v26)
    - `sim_year` is the simulation year (2015-2019)
    - `n_samples` is the number of samples you wish to generate. If using a bucket definition file, which is expected for allocation sampling, this number needs to match the number of samples in the bucket definition file.
    - `n_buckets` is an speed optimization for the allocation sampling approach. If using the testing bucket definition file or not using a bucket allocation file set this value to 1. If using a production bucket definition file set this value to the number of samples per bucket, which is found by looking at the number of repeated values in the production bucket allocation file.
    - `hvac_sizing` dictates whether the HVAC systems in the models are autosized or hardsized (use "autosize" or "hardsize" for this argument).
    - `bucket_definition_file` is the CSV file that specifies which combination of primary attributes need to be sampled and how often they should be sampled.
- This will generate a buildstock.csv file and save it to `.\output-buildstocks\intermediate`.
- The name of the file will include the date, tsv version, simulation year, your username, and number of samples. For example: `buildstock_20221020_v17_2018_alebar_500.csv`.


#### Joining geospatial columns onto the sample using `join_geospatial.py`
After generating the `buildstock.csv` file, you will need to join geospatial columns using `join_geospatial.py`.
- From this directory, run the following command:

    ```
    $ python join_geospatial.py name_of_buildstock.csv
    ```

- Where `name_of_buildstock.csv` is the file generated in the first step. Be sure to include the ".csv" file extension in the command.
- The script will read the file from the `output-buildstocks\intermediate` directory.
- This script will save a file to `output-buildstocks\final` with the same name as the original `buildstock.csv`.

## Adding a new TSV file
To add a new TSV file follow these steps:
1. Unzip the latest vXX TSV file zip in the tsvs directory.
2. Increment the folder name by one so it is unique.
3. Add the new TSV file to the newly renamed folder.
4. Compress the newly renamed folder with the new TSV to create a new version of TSV folder.
5. Open the tsv_sampling.py file and go to the `TSV_ARRAYS` class variable
6. Add the name of the new TSV file to the array of arrays following all dependencies in the TSV. i.e. if the TSV depends on `building_type` and `weekday_start_time` add the new TSV name as a string immediatly following `weekday_start_time`.
7. Do a test run of the `tsv_sampling.py` script to ensure the TSV addition works as expected.

## Special Instructions if changing `hvac_system_type.tsv`, `hvac_system_size_bin.tsv`, or `heating_fuel.tsv`
Both the HVAC System Type and Heating Fuel distributions are used in postprocessing as well as sampling. In addition, these files are used to define the model sample segments which are defined in the `bucket_definition_file` input to the `tsv_sampling.py`. As such, when either of these files are updated the new files must be uploaded to S3, the `bucket_definition_file`(s) updated, and then the output of `tsv_sampling.py` validated against the updated files. These steps are addressed in order.

### Uploading the new files to S3
Both of these files live in the `s3://eulp/truth_data/v01/StockE/` folder in the resbldg account. In that folder both files are versioned (seperately from the TSV file version or EUSS version) and each new added file needs to be added as ther sequential version of said file. For example, if `hvac_system_type_v1.tsv`, `hvac_system_type_v2.tsv`, and `hvac_system_type_v3.tsv` already exist then the updated file should be uploaded as `hvac_system_type_v4.tsv`. Upload the new file(s) with updated version identifiers.

### Updating the `bucket_definition_file`s
The three files discussed in this section are used in the segmentation apportionment sampling process. To support this, slight updates / changes must be made to the `comstock_apportionment.py` file in the postprocessing folder. First, the relevant file names that have been updated and loaded to S3 must be updated in the `Aportion` class' `__init__` method. Ensure each file has the correct updated version identifier and save changes before proceeding.

Next, the `Apportionment` class needs to be loaded and the `generate_sampling_input` method used to create new input bucket definition files. The method is setup to automatically generate files for 1 and 12 samples per segment / bucket, which is the standard at time of writing for test runs and production runs respectivly. To run this code the following must be run in a python interperter which has comstockpostproc installed.

```py
import comstockpostproc as cspp
stock_estimate = cspp.Apportion(
    stock_estimation_version='2024R2',  # Only updated when a new stock estimate is published
    truth_data_version='v01'  # Typically don't change this
)
stock_estimate.generate_sampling_input()
```

This will write two files to this sampling directory, 
